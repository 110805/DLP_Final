{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "BayesianAP_GC_LSTM_UTD.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPto8TarWxW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import glob\n",
        "import scipy.io\n",
        "import os\n",
        "import math\n",
        "\n",
        "from blitz.modules import BayesianLinear, BayesianConv2d, BayesianLSTM\n",
        "from blitz.utils import variational_estimator\n",
        "\n",
        "num_joint = 20\n",
        "max_frame = 125\n",
        "input_feature = 6\n",
        "num_feature = 16\n",
        "hidden_size = 256\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "decay_rate = 0.9\n",
        "decay_step = 100\n",
        "epochs = 1500\n",
        "device = 'cuda:0'\n",
        "path = \"UTD_AP/\"\n",
        "\n",
        "class UTDDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        super(UTDDataset, self).__init__()\n",
        "        self.data_path = data_path\n",
        "        self.load_data()\n",
        "        \n",
        "    def load_data(self):\n",
        "        path_pattern = self.data_path + '*.mat'\n",
        "        files_list = glob.glob(path_pattern, recursive=True)\n",
        "        self.data = torch.zeros((len(files_list),input_feature,max_frame,num_joint),dtype=torch.float32)\n",
        "        self.labels = []\n",
        "        self.num_frame = []\n",
        "        for i,file_name in enumerate(files_list):\n",
        "            a = os.path.basename(file_name).split('_')[0]\n",
        "            mat = scipy.io.loadmat(file_name)['d_skel'].astype(\"float32\")\n",
        "            mat = mat.transpose((1,2,0)) # transpose to (C, #frame, #joint)\n",
        "            mat -= np.expand_dims(mat[:,:,2],axis=2) # set center at spine of body\n",
        "            \n",
        "            aug_mat = np.concatenate((mat,mat),axis=0) # concat speed of xyz-axis\n",
        "            aug_mat[3:,:,:] -= np.roll(mat,1,axis=1) # speed = x(t) - x(t-1)\n",
        "            aug_mat[3:,0,:] = 0 # 1st frame, speed = 0\n",
        "            \n",
        "            #self.data.append(aug_mat)\n",
        "            frame = aug_mat.shape[1]\n",
        "            self.data[i,:,:frame] = torch.from_numpy(aug_mat)\n",
        "            self.num_frame.append(frame)\n",
        "            self.labels.append(int(a[1:])-1)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        data = self.data[index]\n",
        "        label = self.labels[index]\n",
        "        f = self.num_frame[index]\n",
        "        return data, label, f\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    \n",
        "train_dataset = UTDDataset(data_path=path+'train/') # subject 1, 3, 5\n",
        "valid_dataset = UTDDataset(data_path=path+'valid/') # subject 7\n",
        "test_dataset = UTDDataset(data_path=path+'test/')   # subject 2, 4, 6, 8\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=len(test_dataset),shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hse6fvXfWxW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validloader_iter = iter(valid_loader)\n",
        "for i in range(100):\n",
        "    try:\n",
        "        data, label, f = next(validloader_iter)\n",
        "    except StopIteration:\n",
        "        validloader_iter = iter(valid_loader)\n",
        "        data, label, f = next(validloader_iter)\n",
        "    print(f.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0Ijs_HoWxXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "1. head; \n",
        "2. shoulder_center;\n",
        "3. spine;\n",
        "4. hip_center;\n",
        "5. left_shoulder;\n",
        "6. left_elbow;\n",
        "7. left_wrist;\n",
        "8. left_hand;\n",
        "9. right_shoulder;\n",
        "10. right_elbow;\n",
        "11. right_wrist;\n",
        "12. right_hand;\n",
        "13. left_hip;\n",
        "14. left_knee;\n",
        "15. left_ankle;\n",
        "16. left_foot;\n",
        "17. right_hip;\n",
        "18. right_knee;\n",
        "19. right_ankle;\n",
        "20. right_foot;\n",
        "\"\"\"\n",
        "inward_ori_index = [(1,2),(3,2),(4,3),(5,2),(6,5),(7,6),(8,7),(9,2),(10,9),(11,10),\n",
        "                    (12,11),(13,4),(14,13),(15,14),(16,15),(17,4),(18,17),(19,18),(20,19)]\n",
        "inward = [(i - 1, j - 1) for (i, j) in inward_ori_index]\n",
        "outward = [(j, i) for (i, j) in inward]\n",
        "five_key_point = [1,7,11,15,19]\n",
        "\n",
        "def normalize(A):\n",
        "    rowsum = torch.sum(A, 0)\n",
        "    r_inv = torch.pow(rowsum, -0.5)\n",
        "    r_mat_inv = torch.diag(r_inv).float()\n",
        "\n",
        "    A_norm = torch.mm(r_mat_inv, A)\n",
        "    A_norm = torch.mm(A_norm, r_mat_inv)\n",
        "\n",
        "    return A_norm\n",
        "    \n",
        "def gen_adj():\n",
        "    A = torch.zeros(3,num_joint,num_joint,dtype=torch.float)\n",
        "    for (i,j) in inward:\n",
        "        A[0,j,i] = 1\n",
        "    for (i,j) in outward:\n",
        "        A[1,j,i] = 1\n",
        "    for i in five_key_point:\n",
        "        for j in five_key_point:\n",
        "            A[2,i,j] = 1\n",
        "    for i in range(num_joint):\n",
        "        A[:,i,i] = 1\n",
        "    A[0] = normalize(A[0])\n",
        "    A[1] = normalize(A[1])\n",
        "    A[2] = normalize(A[2])\n",
        "    return A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6OGLqGCWxXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.parameter import Parameter\n",
        "class GraphConvolution(nn.Module):\n",
        "    def __init__(self, num_graph, in_feature, out_feature):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.num_graph = num_graph\n",
        "        self.in_feature = in_feature\n",
        "        self.out_feature = out_feature\n",
        "        \n",
        "        self.mask = nn.Parameter(torch.ones(num_graph, num_joint, num_joint))\n",
        "        \n",
        "        self.gcn_list = nn.ModuleList([\n",
        "            BayesianConv2d(\n",
        "                self.in_feature,\n",
        "                self.out_feature,\n",
        "                kernel_size=(1, 1)) for i in range(self.num_graph)\n",
        "        ])\n",
        "        \n",
        "        self.bn = nn.BatchNorm2d(out_feature)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, adj, x):\n",
        "        # x : B*f*T*20\n",
        "        N, C, T, V = x.size()\n",
        "        \n",
        "        adj = adj * self.mask\n",
        "        \n",
        "        for i,a in enumerate(adj):\n",
        "            xa = x.view(-1,V).mm(a).view(N,C,T,V)\n",
        "            if i == 0:\n",
        "                y = self.gcn_list[i](xa)\n",
        "            else:\n",
        "                y += self.gcn_list[i](xa)\n",
        "                \n",
        "        y = self.bn(y)\n",
        "        \n",
        "        return self.act(y)\n",
        "    \n",
        "class GCLayers(nn.Module):\n",
        "    def __init__(self,num_feature,num_graph):\n",
        "        super(GCLayers, self).__init__()\n",
        "        self.num_feature = num_feature\n",
        "        self.gc1 = GraphConvolution(num_graph,input_feature,num_feature)\n",
        "        self.gc2 = GraphConvolution(num_graph,num_feature,num_feature)\n",
        "        self.gc3 = GraphConvolution(num_graph,num_feature,num_feature)\n",
        "        self.gc4 = GraphConvolution(num_graph,num_feature,num_feature)\n",
        "        \n",
        "    def forward(self, adj, x):\n",
        "        # x : B*6*T*20\n",
        "        output1 = self.gc1(adj,x)\n",
        "        output2 = self.gc2(adj,output1)\n",
        "        output3 = self.gc3(adj,output2) + output1\n",
        "        output4 = self.gc4(adj,output3) + output2\n",
        "        return output4\n",
        "\n",
        "@variational_estimator\n",
        "class GC_LSTM(nn.Module):\n",
        "    def __init__(self, num_feature, hidden_size):\n",
        "        super(GC_LSTM, self).__init__()\n",
        "        self.adj = gen_adj().to(device)\n",
        "        self.num_graph = self.adj.shape[0]\n",
        "        self.num_feature = num_feature\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_feature = num_feature\n",
        "        \n",
        "        self.datat_bn = nn.BatchNorm1d(input_feature * num_joint)\n",
        "        self.gclayers = GCLayers(num_feature,self.num_graph)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        #self.lstm = BayesianLSTM(self.output_feature*num_joint,hidden_size,prior_sigma_1=1,prior_pi=1,posterior_rho_init=-3.0)\n",
        "        self.lstm = nn.LSTM(self.output_feature*num_joint,hidden_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, num_frame):\n",
        "        # x : B*6*T*20\n",
        "        x = self.gclayers(self.adj,x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        N,C,T,V = x.size()\n",
        "        x = x.permute(0,2,1,3).contiguous().view(N,T,1,C*V)\n",
        "        for i in range(N):\n",
        "            if i == 0:\n",
        "                output = self.lstm(x[i,:num_frame[i]])[0][-1] # 取lstm最後一個output\n",
        "            else:\n",
        "                output = torch.cat((output,self.lstm(x[i,:num_frame[i]])[0][-1]))\n",
        "        \n",
        "        return self.dropout(output)\n",
        "\n",
        "@variational_estimator\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc = BayesianLinear(hidden_size,27)\n",
        "        self.act = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return self.act(x)\n",
        "    \n",
        "@variational_estimator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc = BayesianLinear(hidden_size,1)\n",
        "        self.act = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return self.act(x)\n",
        "            \n",
        "net = GC_LSTM(num_feature,hidden_size)\n",
        "net = net.to(device)\n",
        "classifier = Classifier(hidden_size)\n",
        "classifier = classifier.to(device)\n",
        "discriminator = Discriminator(hidden_size)\n",
        "discriminator = discriminator.to(device)\n",
        "\n",
        "CE_criterion = nn.CrossEntropyLoss()\n",
        "BCE_criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n",
        "optimizer_C = torch.optim.Adam(classifier.parameters(),lr=learning_rate)\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(),lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=decay_step,gamma=decay_rate)\n",
        "scheduler_C = torch.optim.lr_scheduler.StepLR(optimizer_C,step_size=decay_step,gamma=decay_rate)\n",
        "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D,step_size=decay_step,gamma=decay_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4J06JVbPWxXH",
        "colab_type": "code",
        "colab": {},
        "outputId": "9fbd4e7c-2594-45f9-c814-95c6b0188e5e"
      },
      "source": [
        "kl_weight = 1. / len(train_dataset)\n",
        "\n",
        "M = 10 # for Monte Carlo estimation\n",
        "\n",
        "test_interval = 10\n",
        "early_stop = 0.92\n",
        "training_Dloss = []\n",
        "training_Gloss = []\n",
        "start = time.time()\n",
        "net.train()\n",
        "classifier.train()\n",
        "discriminator.train()\n",
        "\n",
        "validloader_iter = iter(valid_loader)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    G_LOSS = 0\n",
        "    D_LOSS = 0\n",
        "    print(\"{:3d} epoch\".format(epoch+1),end=\", \")\n",
        "    correct = 0\n",
        "    for i,(data, label, num_frame) in enumerate(train_loader):\n",
        "        try:\n",
        "            valid_data, valid_label, valid_f = next(validloader_iter)\n",
        "        except StopIteration:\n",
        "            validloader_iter = iter(valid_loader)\n",
        "            valid_data, valid_label, valid_f = next(validloader_iter)\n",
        "        \n",
        "        data, label, num_frame = data.to(device), label.to(device), num_frame.to(device)\n",
        "        valid_data, valid_label, valid_f = valid_data.to(device), valid_label.to(device), valid_f.to(device)\n",
        "        \n",
        "        positive = torch.ones(label.size()).to(device)\n",
        "        valid_positive = torch.ones(valid_label.size()).to(device)\n",
        "        valid_negative = torch.zeros(valid_label.size()).to(device)\n",
        "        \n",
        "        # train discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        for m in range(M):\n",
        "            feature = net(data,num_frame)\n",
        "            output = discriminator(feature).squeeze()\n",
        "            D_positive_loss = BCE_criterion(output,positive)\n",
        "            \n",
        "            valid_feature = net(valid_data,valid_f)\n",
        "            valid_output = discriminator(valid_feature).squeeze()\n",
        "            D_negative_loss = BCE_criterion(valid_output,valid_negative)\n",
        "            \n",
        "            D_kl_loss = discriminator.nn_kl_divergence() * kl_weight\n",
        "            \n",
        "            D_loss = (D_positive_loss + D_negative_loss + D_kl_loss) / M\n",
        "            \n",
        "            D_loss.backward()\n",
        "            D_LOSS += D_loss.item()\n",
        "        optimizer_D.step()\n",
        "        \n",
        "        # train GC-LSTM and Classifier\n",
        "        optimizer.zero_grad()\n",
        "        optimizer_C.zero_grad()\n",
        "        for m in range(M):\n",
        "            feature = net(data,num_frame)\n",
        "            output = classifier(feature)\n",
        "            class_loss = CE_criterion(output, label)\n",
        "            \n",
        "            valid_feature = net(valid_data,valid_f)\n",
        "            valid_output = discriminator(valid_feature).squeeze()\n",
        "            adversarial_loss = BCE_criterion(valid_output,valid_positive)\n",
        "            \n",
        "            G_kl_loss = net.nn_kl_divergence() * kl_weight\n",
        "            C_kl_loss = classifier.nn_kl_divergence() * kl_weight\n",
        "            \n",
        "            G_loss = (class_loss + adversarial_loss + G_kl_loss + C_kl_loss) / M\n",
        "            G_loss.backward()\n",
        "            G_LOSS += G_loss.item()\n",
        "            \n",
        "            _, pred = output.max(1)\n",
        "            correct += pred.eq(label).sum().item()\n",
        "        optimizer.step()\n",
        "        optimizer_C.step()\n",
        "    correct /= 10\n",
        "    training_Dloss.append(D_LOSS/len(train_dataset))\n",
        "    training_Gloss.append(G_LOSS/len(train_dataset))\n",
        "    print(\"D loss:{:6.4f}, G loss:{:6.4f}, training acc:{:6.2f}%, time:{:.2f}s\"\n",
        "          .format(training_Dloss[-1],training_Gloss[-1],correct/len(train_dataset)*100.,time.time()-start))\n",
        "\n",
        "    scheduler.step()\n",
        "    scheduler_C.step()\n",
        "    scheduler_D.step()\n",
        "    if (epoch+1) % test_interval == 0:\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for (data, label, num_frame) in test_loader:\n",
        "                data, label, num_frame = data.to(device), label.to(device), num_frame.to(device)\n",
        "                for m in range(M):\n",
        "                    feature = net(data,num_frame)\n",
        "                    output = classifier(feature)\n",
        "                    _, pred = output.max(1)\n",
        "                    correct += pred.eq(label).sum().item()\n",
        "        correct /= 10\n",
        "        print(\"test acc: {:5.2f}%, time:{:7.2f}s\"\n",
        "              .format(correct/len(test_dataset)*100.,time.time()-start))\n",
        "        if correct/len(test_dataset) > early_stop:\n",
        "            torch.save(net,\"Bayesian_GC_LSTM.pkl\")\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1 epoch, D loss:0.2258, G loss:0.1248, training acc:  3.37%, time:26.56s\n",
            "  2 epoch, D loss:0.2692, G loss:0.1214, training acc:  6.16%, time:52.48s\n",
            "  3 epoch, D loss:0.2304, G loss:0.1213, training acc:  5.26%, time:78.39s\n",
            "  4 epoch, D loss:0.1931, G loss:0.1211, training acc:  6.81%, time:104.35s\n",
            "  5 epoch, D loss:0.1644, G loss:0.1212, training acc:  7.21%, time:130.85s\n",
            "  6 epoch, D loss:0.1523, G loss:0.1211, training acc:  7.62%, time:156.74s\n",
            "  7 epoch, D loss:0.1563, G loss:0.1212, training acc:  6.56%, time:182.62s\n",
            "  8 epoch, D loss:0.1584, G loss:0.1209, training acc:  7.00%, time:208.63s\n",
            "  9 epoch, D loss:0.1551, G loss:0.1210, training acc:  7.52%, time:235.24s\n",
            " 10 epoch, D loss:0.1299, G loss:0.1214, training acc:  7.40%, time:261.43s\n",
            "test acc:  8.33%, time: 263.99s\n",
            " 11 epoch, D loss:0.1072, G loss:0.1222, training acc:  8.30%, time:289.94s\n",
            " 12 epoch, D loss:0.0943, G loss:0.1235, training acc:  7.21%, time:316.22s\n",
            " 13 epoch, D loss:0.0847, G loss:0.1245, training acc:  6.87%, time:343.07s\n",
            " 14 epoch, D loss:0.0761, G loss:0.1257, training acc:  5.79%, time:369.18s\n",
            " 15 epoch, D loss:0.0768, G loss:0.1254, training acc:  6.04%, time:395.24s\n",
            " 16 epoch, D loss:0.0720, G loss:0.1263, training acc:  7.00%, time:421.43s\n",
            " 17 epoch, D loss:0.0761, G loss:0.1256, training acc:  6.97%, time:448.07s\n",
            " 18 epoch, D loss:0.0825, G loss:0.1245, training acc:  5.76%, time:474.25s\n",
            " 19 epoch, D loss:0.0823, G loss:0.1246, training acc:  6.56%, time:500.30s\n",
            " 20 epoch, D loss:0.0981, G loss:0.1226, training acc:  6.38%, time:526.49s\n",
            "test acc:  8.09%, time: 529.04s\n",
            " 21 epoch, D loss:0.0991, G loss:0.1227, training acc:  7.06%, time:555.71s\n",
            " 22 epoch, D loss:0.0857, G loss:0.1243, training acc:  7.89%, time:581.74s\n",
            " 23 epoch, D loss:0.0803, G loss:0.1250, training acc:  7.24%, time:608.07s\n",
            " 24 epoch, D loss:0.0936, G loss:0.1230, training acc:  6.59%, time:634.20s\n",
            " 25 epoch, D loss:0.0951, G loss:0.1227, training acc:  7.06%, time:660.97s\n",
            " 26 epoch, D loss:0.0977, G loss:0.1221, training acc:  9.01%, time:687.19s\n",
            " 27 epoch, D loss:0.1027, G loss:0.1207, training acc: 14.52%, time:713.27s\n",
            " 28 epoch, D loss:0.0860, G loss:0.1222, training acc: 16.66%, time:739.39s\n",
            " 29 epoch, D loss:0.0642, G loss:0.1263, training acc: 17.09%, time:766.05s\n",
            " 30 epoch, D loss:0.0573, G loss:0.1294, training acc: 15.63%, time:792.16s\n",
            "test acc: 15.47%, time: 794.71s\n",
            " 31 epoch, D loss:0.0595, G loss:0.1278, training acc: 15.08%, time:821.01s\n",
            " 32 epoch, D loss:0.0608, G loss:0.1269, training acc: 17.68%, time:847.10s\n",
            " 33 epoch, D loss:0.0555, G loss:0.1295, training acc: 17.77%, time:873.65s\n",
            " 34 epoch, D loss:0.0551, G loss:0.1288, training acc: 18.45%, time:899.69s\n",
            " 35 epoch, D loss:0.0588, G loss:0.1269, training acc: 15.70%, time:925.70s\n",
            " 36 epoch, D loss:0.0562, G loss:0.1286, training acc: 14.46%, time:951.81s\n",
            " 37 epoch, D loss:0.0528, G loss:0.1307, training acc: 18.48%, time:978.28s\n",
            " 38 epoch, D loss:0.0525, G loss:0.1311, training acc: 19.01%, time:1004.09s\n",
            " 39 epoch, D loss:0.0517, G loss:0.1315, training acc: 19.47%, time:1030.13s\n",
            " 40 epoch, D loss:0.0518, G loss:0.1320, training acc: 18.67%, time:1056.23s\n",
            "test acc: 18.79%, time:1059.11s\n",
            " 41 epoch, D loss:0.0516, G loss:0.1313, training acc: 20.12%, time:1085.76s\n",
            " 42 epoch, D loss:0.0503, G loss:0.1323, training acc: 26.41%, time:1111.82s\n",
            " 43 epoch, D loss:0.0503, G loss:0.1318, training acc: 28.95%, time:1138.02s\n",
            " 44 epoch, D loss:0.0503, G loss:0.1313, training acc: 30.12%, time:1163.94s\n",
            " 45 epoch, D loss:0.0506, G loss:0.1305, training acc: 31.67%, time:1190.61s\n",
            " 46 epoch, D loss:0.0501, G loss:0.1307, training acc: 33.25%, time:1216.71s\n",
            " 47 epoch, D loss:0.0496, G loss:0.1305, training acc: 34.95%, time:1242.70s\n",
            " 48 epoch, D loss:0.0490, G loss:0.1338, training acc: 30.80%, time:1268.85s\n",
            " 49 epoch, D loss:0.0489, G loss:0.1329, training acc: 31.49%, time:1295.42s\n",
            " 50 epoch, D loss:0.0488, G loss:0.1314, training acc: 31.46%, time:1321.30s\n",
            "test acc: 28.53%, time:1324.16s\n",
            " 51 epoch, D loss:0.0492, G loss:0.1314, training acc: 35.57%, time:1350.27s\n",
            " 52 epoch, D loss:0.0494, G loss:0.1324, training acc: 36.53%, time:1376.24s\n",
            " 53 epoch, D loss:0.0496, G loss:0.1307, training acc: 37.62%, time:1402.79s\n",
            " 54 epoch, D loss:0.0497, G loss:0.1314, training acc: 37.34%, time:1428.84s\n",
            " 55 epoch, D loss:0.0497, G loss:0.1306, training acc: 37.93%, time:1454.90s\n",
            " 56 epoch, D loss:0.0491, G loss:0.1307, training acc: 37.52%, time:1480.88s\n",
            " 57 epoch, D loss:0.0496, G loss:0.1306, training acc: 37.28%, time:1507.40s\n",
            " 58 epoch, D loss:0.0502, G loss:0.1280, training acc: 37.46%, time:1533.42s\n",
            " 59 epoch, D loss:0.0503, G loss:0.1295, training acc: 37.65%, time:1559.50s\n",
            " 60 epoch, D loss:0.0499, G loss:0.1284, training acc: 40.65%, time:1585.66s\n",
            "test acc: 35.33%, time:1588.20s\n",
            " 61 epoch, D loss:0.0493, G loss:0.1299, training acc: 42.79%, time:1614.85s\n",
            " 62 epoch, D loss:0.0496, G loss:0.1278, training acc: 43.72%, time:1641.00s\n",
            " 63 epoch, D loss:0.0498, G loss:0.1270, training acc: 43.72%, time:1666.93s\n",
            " 64 epoch, D loss:0.0500, G loss:0.1273, training acc: 42.32%, time:1693.13s\n",
            " 65 epoch, D loss:0.0502, G loss:0.1276, training acc: 41.70%, time:1719.74s\n",
            " 66 epoch, D loss:0.0498, G loss:0.1269, training acc: 44.30%, time:1745.79s\n",
            " 67 epoch, D loss:0.0501, G loss:0.1260, training acc: 46.75%, time:1771.83s\n",
            " 68 epoch, D loss:0.0489, G loss:0.1289, training acc: 43.37%, time:1797.81s\n",
            " 69 epoch, D loss:0.0497, G loss:0.1259, training acc: 42.32%, time:1824.51s\n",
            " 70 epoch, D loss:0.0496, G loss:0.1280, training acc: 43.22%, time:1850.71s\n",
            "test acc: 36.00%, time:1853.26s\n",
            " 71 epoch, D loss:0.0489, G loss:0.1303, training acc: 41.52%, time:1879.29s\n",
            " 72 epoch, D loss:0.0486, G loss:0.1282, training acc: 45.79%, time:1905.23s\n",
            " 73 epoch, D loss:0.0484, G loss:0.1270, training acc: 46.10%, time:1931.82s\n",
            " 74 epoch, D loss:0.0484, G loss:0.1274, training acc: 45.14%, time:1957.87s\n",
            " 75 epoch, D loss:0.0485, G loss:0.1290, training acc: 46.35%, time:1983.97s\n",
            " 76 epoch, D loss:0.0486, G loss:0.1280, training acc: 46.93%, time:2010.04s\n",
            " 77 epoch, D loss:0.0485, G loss:0.1311, training acc: 40.34%, time:2036.75s\n",
            " 78 epoch, D loss:0.0488, G loss:0.1281, training acc: 47.31%, time:2062.71s\n",
            " 79 epoch, D loss:0.0485, G loss:0.1284, training acc: 48.27%, time:2088.68s\n",
            " 80 epoch, D loss:0.0484, G loss:0.1283, training acc: 52.41%, time:2114.85s\n",
            "test acc: 43.74%, time:2117.51s\n",
            " 81 epoch, D loss:0.0482, G loss:0.1267, training acc: 53.62%, time:2144.11s\n",
            " 82 epoch, D loss:0.0482, G loss:0.1255, training acc: 55.67%, time:2170.14s\n",
            " 83 epoch, D loss:0.0485, G loss:0.1260, training acc: 54.30%, time:2196.25s\n",
            " 84 epoch, D loss:0.0485, G loss:0.1267, training acc: 55.76%, time:2222.40s\n",
            " 85 epoch, D loss:0.0487, G loss:0.1244, training acc: 54.98%, time:2249.03s\n",
            " 86 epoch, D loss:0.0486, G loss:0.1257, training acc: 57.83%, time:2275.01s\n",
            " 87 epoch, D loss:0.0485, G loss:0.1240, training acc: 57.40%, time:2301.20s\n",
            " 88 epoch, D loss:0.0485, G loss:0.1246, training acc: 59.01%, time:2327.35s\n",
            " 89 epoch, D loss:0.0488, G loss:0.1251, training acc: 56.69%, time:2353.85s\n",
            " 90 epoch, D loss:0.0487, G loss:0.1246, training acc: 56.22%, time:2379.83s\n",
            "test acc: 42.79%, time:2382.38s\n",
            " 91 epoch, D loss:0.0487, G loss:0.1259, training acc: 50.28%, time:2408.50s\n",
            " 92 epoch, D loss:0.0483, G loss:0.1255, training acc: 56.56%, time:2434.67s\n",
            " 93 epoch, D loss:0.0482, G loss:0.1249, training acc: 58.39%, time:2461.30s\n",
            " 94 epoch, D loss:0.0483, G loss:0.1241, training acc: 58.61%, time:2487.32s\n",
            " 95 epoch, D loss:0.0483, G loss:0.1258, training acc: 55.82%, time:2513.42s\n",
            " 96 epoch, D loss:0.0480, G loss:0.1251, training acc: 59.88%, time:2539.41s\n",
            " 97 epoch, D loss:0.0481, G loss:0.1237, training acc: 62.26%, time:2566.02s\n",
            " 98 epoch, D loss:0.0485, G loss:0.1230, training acc: 63.34%, time:2592.01s\n",
            " 99 epoch, D loss:0.0484, G loss:0.1246, training acc: 60.87%, time:2618.10s\n",
            "100 epoch, D loss:0.0483, G loss:0.1255, training acc: 62.26%, time:2644.10s\n",
            "test acc: 47.44%, time:2646.65s\n",
            "101 epoch, D loss:0.0486, G loss:0.1214, training acc: 63.75%, time:2673.21s\n",
            "102 epoch, D loss:0.0482, G loss:0.1220, training acc: 64.49%, time:2699.27s\n",
            "103 epoch, D loss:0.0483, G loss:0.1220, training acc: 65.20%, time:2725.37s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "104 epoch, D loss:0.0485, G loss:0.1216, training acc: 64.92%, time:2751.35s\n",
            "105 epoch, D loss:0.0484, G loss:0.1211, training acc: 66.50%, time:2777.94s\n",
            "106 epoch, D loss:0.0485, G loss:0.1236, training acc: 61.95%, time:2803.95s\n",
            "107 epoch, D loss:0.0481, G loss:0.1247, training acc: 61.02%, time:2829.94s\n",
            "108 epoch, D loss:0.0482, G loss:0.1224, training acc: 65.54%, time:2856.00s\n",
            "109 epoch, D loss:0.0477, G loss:0.1244, training acc: 65.39%, time:2882.44s\n",
            "110 epoch, D loss:0.0477, G loss:0.1229, training acc: 67.46%, time:2908.45s\n",
            "test acc: 50.67%, time:2911.00s\n",
            "111 epoch, D loss:0.0478, G loss:0.1215, training acc: 66.19%, time:2936.99s\n",
            "112 epoch, D loss:0.0484, G loss:0.1225, training acc: 66.04%, time:2962.95s\n",
            "113 epoch, D loss:0.0485, G loss:0.1216, training acc: 66.28%, time:2989.53s\n",
            "114 epoch, D loss:0.0482, G loss:0.1215, training acc: 68.98%, time:3015.44s\n",
            "115 epoch, D loss:0.0484, G loss:0.1209, training acc: 68.92%, time:3041.44s\n",
            "116 epoch, D loss:0.0485, G loss:0.1197, training acc: 70.19%, time:3067.37s\n",
            "117 epoch, D loss:0.0492, G loss:0.1216, training acc: 64.71%, time:3093.90s\n",
            "118 epoch, D loss:0.0487, G loss:0.1205, training acc: 66.81%, time:3119.95s\n",
            "119 epoch, D loss:0.0482, G loss:0.1212, training acc: 68.33%, time:3145.99s\n",
            "120 epoch, D loss:0.0481, G loss:0.1208, training acc: 67.83%, time:3172.10s\n",
            "test acc: 50.51%, time:3174.65s\n",
            "121 epoch, D loss:0.0484, G loss:0.1192, training acc: 70.31%, time:3201.23s\n",
            "122 epoch, D loss:0.0485, G loss:0.1212, training acc: 67.96%, time:3227.17s\n",
            "123 epoch, D loss:0.0486, G loss:0.1199, training acc: 67.86%, time:3253.26s\n",
            "124 epoch, D loss:0.0486, G loss:0.1200, training acc: 69.94%, time:3279.19s\n",
            "125 epoch, D loss:0.0483, G loss:0.1196, training acc: 70.03%, time:3305.67s\n",
            "126 epoch, D loss:0.0484, G loss:0.1203, training acc: 66.97%, time:3331.68s\n",
            "127 epoch, D loss:0.0487, G loss:0.1192, training acc: 69.13%, time:3357.54s\n",
            "128 epoch, D loss:0.0486, G loss:0.1199, training acc: 70.53%, time:3383.55s\n",
            "129 epoch, D loss:0.0483, G loss:0.1179, training acc: 70.43%, time:3410.11s\n",
            "130 epoch, D loss:0.0484, G loss:0.1186, training acc: 72.63%, time:3436.18s\n",
            "test acc: 58.21%, time:3438.72s\n",
            "131 epoch, D loss:0.0485, G loss:0.1192, training acc: 73.68%, time:3464.75s\n",
            "132 epoch, D loss:0.0484, G loss:0.1181, training acc: 74.58%, time:3490.64s\n",
            "133 epoch, D loss:0.0485, G loss:0.1166, training acc: 75.94%, time:3517.30s\n",
            "134 epoch, D loss:0.0483, G loss:0.1182, training acc: 75.29%, time:3543.30s\n",
            "135 epoch, D loss:0.0484, G loss:0.1173, training acc: 75.33%, time:3569.20s\n",
            "136 epoch, D loss:0.0487, G loss:0.1181, training acc: 76.13%, time:3595.13s\n",
            "137 epoch, D loss:0.0485, G loss:0.1175, training acc: 77.52%, time:3621.71s\n",
            "138 epoch, D loss:0.0485, G loss:0.1191, training acc: 76.81%, time:3647.68s\n",
            "139 epoch, D loss:0.0486, G loss:0.1186, training acc: 76.75%, time:3673.60s\n",
            "140 epoch, D loss:0.0482, G loss:0.1198, training acc: 77.34%, time:3699.60s\n",
            "test acc: 59.51%, time:3702.19s\n",
            "141 epoch, D loss:0.0483, G loss:0.1176, training acc: 76.41%, time:3728.63s\n",
            "142 epoch, D loss:0.0483, G loss:0.1167, training acc: 76.47%, time:3754.82s\n",
            "143 epoch, D loss:0.0484, G loss:0.1171, training acc: 74.18%, time:3780.98s\n",
            "144 epoch, D loss:0.0487, G loss:0.1173, training acc: 75.70%, time:3807.19s\n",
            "145 epoch, D loss:0.0486, G loss:0.1159, training acc: 76.53%, time:3833.77s\n",
            "146 epoch, D loss:0.0483, G loss:0.1178, training acc: 76.13%, time:3859.80s\n",
            "147 epoch, D loss:0.0486, G loss:0.1161, training acc: 76.16%, time:3885.75s\n",
            "148 epoch, D loss:0.0484, G loss:0.1171, training acc: 75.88%, time:3911.79s\n",
            "149 epoch, D loss:0.0489, G loss:0.1166, training acc: 76.41%, time:3938.31s\n",
            "150 epoch, D loss:0.0487, G loss:0.1172, training acc: 76.97%, time:3964.45s\n",
            "test acc: 60.72%, time:3967.00s\n",
            "151 epoch, D loss:0.0486, G loss:0.1161, training acc: 77.28%, time:3993.01s\n",
            "152 epoch, D loss:0.0486, G loss:0.1157, training acc: 77.03%, time:4018.96s\n",
            "153 epoch, D loss:0.0491, G loss:0.1166, training acc: 75.42%, time:4045.41s\n",
            "154 epoch, D loss:0.0481, G loss:0.1241, training acc: 63.16%, time:4071.41s\n",
            "155 epoch, D loss:0.0485, G loss:0.1214, training acc: 66.16%, time:4097.52s\n",
            "156 epoch, D loss:0.0485, G loss:0.1196, training acc: 70.53%, time:4123.45s\n",
            "157 epoch, D loss:0.0481, G loss:0.1180, training acc: 71.64%, time:4150.03s\n",
            "158 epoch, D loss:0.0481, G loss:0.1183, training acc: 73.84%, time:4176.05s\n",
            "159 epoch, D loss:0.0484, G loss:0.1181, training acc: 75.79%, time:4202.15s\n",
            "160 epoch, D loss:0.0488, G loss:0.1178, training acc: 73.59%, time:4228.18s\n",
            "test acc: 57.53%, time:4230.81s\n",
            "161 epoch, D loss:0.0483, G loss:0.1189, training acc: 75.70%, time:4257.37s\n",
            "162 epoch, D loss:0.0483, G loss:0.1178, training acc: 77.09%, time:4283.42s\n",
            "163 epoch, D loss:0.0486, G loss:0.1163, training acc: 77.00%, time:4309.44s\n",
            "164 epoch, D loss:0.0485, G loss:0.1162, training acc: 77.12%, time:4335.39s\n",
            "165 epoch, D loss:0.0484, G loss:0.1166, training acc: 77.09%, time:4362.08s\n",
            "166 epoch, D loss:0.0482, G loss:0.1181, training acc: 77.21%, time:4388.07s\n",
            "167 epoch, D loss:0.0483, G loss:0.1169, training acc: 74.06%, time:4414.19s\n",
            "168 epoch, D loss:0.0490, G loss:0.1149, training acc: 75.02%, time:4440.32s\n",
            "169 epoch, D loss:0.0491, G loss:0.1173, training acc: 76.04%, time:4466.79s\n",
            "170 epoch, D loss:0.0485, G loss:0.1173, training acc: 77.37%, time:4492.72s\n",
            "test acc: 60.33%, time:4495.27s\n",
            "171 epoch, D loss:0.0484, G loss:0.1168, training acc: 76.25%, time:4521.27s\n",
            "172 epoch, D loss:0.0487, G loss:0.1149, training acc: 77.24%, time:4547.31s\n",
            "173 epoch, D loss:0.0487, G loss:0.1151, training acc: 77.93%, time:4573.82s\n",
            "174 epoch, D loss:0.0485, G loss:0.1150, training acc: 77.89%, time:4599.98s\n",
            "175 epoch, D loss:0.0489, G loss:0.1154, training acc: 78.33%, time:4626.08s\n",
            "176 epoch, D loss:0.0483, G loss:0.1162, training acc: 77.93%, time:4651.97s\n",
            "177 epoch, D loss:0.0480, G loss:0.1157, training acc: 78.89%, time:4678.50s\n",
            "178 epoch, D loss:0.0480, G loss:0.1150, training acc: 80.87%, time:4704.57s\n",
            "179 epoch, D loss:0.0486, G loss:0.1128, training acc: 83.28%, time:4730.65s\n",
            "180 epoch, D loss:0.0485, G loss:0.1155, training acc: 82.01%, time:4756.59s\n",
            "test acc: 63.65%, time:4759.14s\n",
            "181 epoch, D loss:0.0487, G loss:0.1146, training acc: 83.84%, time:4785.59s\n",
            "182 epoch, D loss:0.0484, G loss:0.1139, training acc: 83.99%, time:4811.58s\n",
            "183 epoch, D loss:0.0490, G loss:0.1124, training acc: 84.21%, time:4837.59s\n",
            "184 epoch, D loss:0.0486, G loss:0.1136, training acc: 83.81%, time:4863.72s\n",
            "185 epoch, D loss:0.0483, G loss:0.1143, training acc: 82.72%, time:4890.43s\n",
            "186 epoch, D loss:0.0483, G loss:0.1151, training acc: 83.62%, time:4916.32s\n",
            "187 epoch, D loss:0.0489, G loss:0.1131, training acc: 82.88%, time:4942.20s\n",
            "188 epoch, D loss:0.0485, G loss:0.1155, training acc: 83.31%, time:4968.22s\n",
            "189 epoch, D loss:0.0485, G loss:0.1134, training acc: 84.67%, time:4994.68s\n",
            "190 epoch, D loss:0.0488, G loss:0.1124, training acc: 83.84%, time:5020.63s\n",
            "test acc: 64.67%, time:5023.29s\n",
            "191 epoch, D loss:0.0489, G loss:0.1142, training acc: 81.86%, time:5049.17s\n",
            "192 epoch, D loss:0.0489, G loss:0.1128, training acc: 85.23%, time:5075.36s\n",
            "193 epoch, D loss:0.0489, G loss:0.1139, training acc: 83.81%, time:5101.94s\n",
            "194 epoch, D loss:0.0488, G loss:0.1129, training acc: 84.40%, time:5127.90s\n",
            "195 epoch, D loss:0.0486, G loss:0.1144, training acc: 79.75%, time:5153.85s\n",
            "196 epoch, D loss:0.0486, G loss:0.1153, training acc: 81.08%, time:5179.82s\n",
            "197 epoch, D loss:0.0487, G loss:0.1140, training acc: 81.55%, time:5206.17s\n",
            "198 epoch, D loss:0.0484, G loss:0.1140, training acc: 83.53%, time:5232.07s\n",
            "199 epoch, D loss:0.0491, G loss:0.1126, training acc: 84.80%, time:5257.96s\n",
            "200 epoch, D loss:0.0496, G loss:0.1117, training acc: 84.55%, time:5283.82s\n",
            "test acc: 66.19%, time:5286.38s\n",
            "201 epoch, D loss:0.0491, G loss:0.1139, training acc: 84.33%, time:5312.89s\n",
            "202 epoch, D loss:0.0489, G loss:0.1134, training acc: 85.54%, time:5338.80s\n",
            "203 epoch, D loss:0.0488, G loss:0.1130, training acc: 85.63%, time:5364.63s\n",
            "204 epoch, D loss:0.0488, G loss:0.1122, training acc: 85.36%, time:5390.48s\n",
            "205 epoch, D loss:0.0488, G loss:0.1125, training acc: 86.38%, time:5416.88s\n",
            "206 epoch, D loss:0.0486, G loss:0.1123, training acc: 86.10%, time:5442.84s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "207 epoch, D loss:0.0488, G loss:0.1129, training acc: 85.08%, time:5468.69s\n",
            "208 epoch, D loss:0.0482, G loss:0.1140, training acc: 83.81%, time:5494.61s\n",
            "209 epoch, D loss:0.0486, G loss:0.1142, training acc: 78.30%, time:5521.10s\n",
            "210 epoch, D loss:0.0491, G loss:0.1129, training acc: 82.79%, time:5547.10s\n",
            "test acc: 66.74%, time:5549.66s\n",
            "211 epoch, D loss:0.0488, G loss:0.1137, training acc: 82.97%, time:5575.83s\n",
            "212 epoch, D loss:0.0485, G loss:0.1129, training acc: 84.24%, time:5601.79s\n",
            "213 epoch, D loss:0.0488, G loss:0.1124, training acc: 83.99%, time:5628.33s\n",
            "214 epoch, D loss:0.0492, G loss:0.1115, training acc: 86.04%, time:5654.35s\n",
            "215 epoch, D loss:0.0495, G loss:0.1128, training acc: 84.92%, time:5680.41s\n",
            "216 epoch, D loss:0.0495, G loss:0.1121, training acc: 86.35%, time:5706.34s\n",
            "217 epoch, D loss:0.0493, G loss:0.1124, training acc: 86.38%, time:5732.98s\n",
            "218 epoch, D loss:0.0487, G loss:0.1143, training acc: 85.63%, time:5758.81s\n",
            "219 epoch, D loss:0.0491, G loss:0.1124, training acc: 85.76%, time:5784.65s\n",
            "220 epoch, D loss:0.0500, G loss:0.1111, training acc: 84.89%, time:5810.57s\n",
            "test acc: 68.30%, time:5813.14s\n",
            "221 epoch, D loss:0.0495, G loss:0.1124, training acc: 84.95%, time:5839.66s\n",
            "222 epoch, D loss:0.0487, G loss:0.1126, training acc: 85.23%, time:5865.42s\n",
            "223 epoch, D loss:0.0498, G loss:0.1106, training acc: 85.20%, time:5891.24s\n",
            "224 epoch, D loss:0.0501, G loss:0.1115, training acc: 85.82%, time:5916.99s\n",
            "225 epoch, D loss:0.0490, G loss:0.1136, training acc: 86.75%, time:5943.25s\n",
            "226 epoch, D loss:0.0486, G loss:0.1120, training acc: 86.93%, time:5968.97s\n",
            "227 epoch, D loss:0.0483, G loss:0.1099, training acc: 87.77%, time:5994.71s\n",
            "228 epoch, D loss:0.0501, G loss:0.1084, training acc: 87.34%, time:6020.57s\n",
            "229 epoch, D loss:0.0506, G loss:0.1094, training acc: 86.93%, time:6046.87s\n",
            "230 epoch, D loss:0.0502, G loss:0.1114, training acc: 86.63%, time:6072.60s\n",
            "test acc: 62.70%, time:6075.16s\n",
            "231 epoch, D loss:0.0497, G loss:0.1113, training acc: 86.63%, time:6101.10s\n",
            "232 epoch, D loss:0.0495, G loss:0.1107, training acc: 86.63%, time:6127.06s\n",
            "233 epoch, D loss:0.0501, G loss:0.1094, training acc: 86.78%, time:6153.86s\n",
            "234 epoch, D loss:0.0508, G loss:0.1097, training acc: 87.00%, time:6180.21s\n",
            "235 epoch, D loss:0.0520, G loss:0.1090, training acc: 84.83%, time:6206.56s\n",
            "236 epoch, D loss:0.0512, G loss:0.1097, training acc: 87.12%, time:6233.03s\n",
            "237 epoch, D loss:0.0524, G loss:0.1084, training acc: 85.73%, time:6259.96s\n",
            "238 epoch, D loss:0.0516, G loss:0.1086, training acc: 86.53%, time:6286.34s\n",
            "239 epoch, D loss:0.0519, G loss:0.1084, training acc: 86.78%, time:6312.64s\n",
            "240 epoch, D loss:0.0517, G loss:0.1103, training acc: 86.50%, time:6338.97s\n",
            "test acc: 67.05%, time:6341.56s\n",
            "241 epoch, D loss:0.0516, G loss:0.1099, training acc: 85.17%, time:6368.46s\n",
            "242 epoch, D loss:0.0524, G loss:0.1089, training acc: 87.89%, time:6394.69s\n",
            "243 epoch, D loss:0.0517, G loss:0.1105, training acc: 85.67%, time:6421.19s\n",
            "244 epoch, D loss:0.0505, G loss:0.1100, training acc: 85.05%, time:6447.55s\n",
            "245 epoch, D loss:0.0514, G loss:0.1088, training acc: 86.25%, time:6474.49s\n",
            "246 epoch, D loss:0.0515, G loss:0.1096, training acc: 86.25%, time:6500.88s\n",
            "247 epoch, D loss:0.0512, G loss:0.1085, training acc: 87.74%, time:6527.16s\n",
            "248 epoch, D loss:0.0504, G loss:0.1085, training acc: 88.33%, time:6553.54s\n",
            "249 epoch, D loss:0.0519, G loss:0.1066, training acc: 87.18%, time:6580.43s\n",
            "250 epoch, D loss:0.0515, G loss:0.1093, training acc: 87.68%, time:6606.75s\n",
            "test acc: 66.95%, time:6609.34s\n",
            "251 epoch, D loss:0.0499, G loss:0.1102, training acc: 88.58%, time:6635.65s\n",
            "252 epoch, D loss:0.0492, G loss:0.1105, training acc: 87.37%, time:6662.00s\n",
            "253 epoch, D loss:0.0492, G loss:0.1097, training acc: 87.62%, time:6688.85s\n",
            "254 epoch, D loss:0.0497, G loss:0.1097, training acc: 87.28%, time:6715.04s\n",
            "255 epoch, D loss:0.0494, G loss:0.1101, training acc: 87.43%, time:6741.50s\n",
            "256 epoch, D loss:0.0498, G loss:0.1096, training acc: 87.62%, time:6767.89s\n",
            "257 epoch, D loss:0.0517, G loss:0.1078, training acc: 88.05%, time:6794.85s\n",
            "258 epoch, D loss:0.0507, G loss:0.1091, training acc: 87.99%, time:6821.05s\n",
            "259 epoch, D loss:0.0504, G loss:0.1114, training acc: 86.78%, time:6847.48s\n",
            "260 epoch, D loss:0.0514, G loss:0.1102, training acc: 87.93%, time:6873.86s\n",
            "test acc: 66.21%, time:6876.45s\n",
            "261 epoch, D loss:0.0510, G loss:0.1098, training acc: 87.80%, time:6903.42s\n",
            "262 epoch, D loss:0.0509, G loss:0.1098, training acc: 88.64%, time:6929.86s\n",
            "263 epoch, D loss:0.0507, G loss:0.1103, training acc: 88.64%, time:6956.29s\n",
            "264 epoch, D loss:0.0506, G loss:0.1090, training acc: 88.58%, time:6982.85s\n",
            "265 epoch, D loss:0.0516, G loss:0.1089, training acc: 88.27%, time:7009.98s\n",
            "266 epoch, D loss:0.0520, G loss:0.1093, training acc: 88.36%, time:7036.45s\n",
            "267 epoch, D loss:0.0506, G loss:0.1115, training acc: 88.64%, time:7062.98s\n",
            "268 epoch, D loss:0.0507, G loss:0.1097, training acc: 88.27%, time:7089.43s\n",
            "269 epoch, D loss:0.0514, G loss:0.1081, training acc: 88.70%, time:7116.35s\n",
            "270 epoch, D loss:0.0512, G loss:0.1098, training acc: 88.70%, time:7142.77s\n",
            "test acc: 67.02%, time:7145.37s\n",
            "271 epoch, D loss:0.0527, G loss:0.1072, training acc: 88.67%, time:7171.59s\n",
            "272 epoch, D loss:0.0536, G loss:0.1070, training acc: 88.33%, time:7197.89s\n",
            "273 epoch, D loss:0.0526, G loss:0.1086, training acc: 88.20%, time:7224.67s\n",
            "274 epoch, D loss:0.0536, G loss:0.1068, training acc: 88.79%, time:7250.99s\n",
            "275 epoch, D loss:0.0514, G loss:0.1079, training acc: 88.14%, time:7277.28s\n",
            "276 epoch, D loss:0.0537, G loss:0.1061, training acc: 88.42%, time:7303.49s\n",
            "277 epoch, D loss:0.0542, G loss:0.1061, training acc: 87.77%, time:7330.19s\n",
            "278 epoch, D loss:0.0558, G loss:0.1068, training acc: 88.11%, time:7356.35s\n",
            "279 epoch, D loss:0.0573, G loss:0.1060, training acc: 87.89%, time:7382.47s\n",
            "280 epoch, D loss:0.0548, G loss:0.1070, training acc: 88.58%, time:7408.64s\n",
            "test acc: 64.63%, time:7411.24s\n",
            "281 epoch, D loss:0.0562, G loss:0.1046, training acc: 88.08%, time:7438.08s\n",
            "282 epoch, D loss:0.0557, G loss:0.1057, training acc: 88.58%, time:7464.22s\n",
            "283 epoch, D loss:0.0524, G loss:0.1091, training acc: 88.76%, time:7490.40s\n",
            "284 epoch, D loss:0.0526, G loss:0.1075, training acc: 88.30%, time:7516.49s\n",
            "285 epoch, D loss:0.0538, G loss:0.1064, training acc: 87.65%, time:7543.20s\n",
            "286 epoch, D loss:0.0533, G loss:0.1076, training acc: 88.24%, time:7569.40s\n",
            "287 epoch, D loss:0.0525, G loss:0.1077, training acc: 88.58%, time:7595.49s\n",
            "288 epoch, D loss:0.0534, G loss:0.1067, training acc: 88.51%, time:7621.63s\n",
            "289 epoch, D loss:0.0566, G loss:0.1037, training acc: 87.09%, time:7648.34s\n",
            "290 epoch, D loss:0.0553, G loss:0.1076, training acc: 87.83%, time:7674.51s\n",
            "test acc: 65.40%, time:7677.11s\n",
            "291 epoch, D loss:0.0526, G loss:0.1086, training acc: 87.86%, time:7703.38s\n",
            "292 epoch, D loss:0.0541, G loss:0.1068, training acc: 87.74%, time:7729.51s\n",
            "293 epoch, D loss:0.0532, G loss:0.1072, training acc: 87.83%, time:7756.16s\n",
            "294 epoch, D loss:0.0526, G loss:0.1093, training acc: 87.93%, time:7782.21s\n",
            "295 epoch, D loss:0.0515, G loss:0.1111, training acc: 87.40%, time:7808.29s\n",
            "296 epoch, D loss:0.0530, G loss:0.1080, training acc: 87.99%, time:7834.39s\n",
            "297 epoch, D loss:0.0539, G loss:0.1073, training acc: 85.48%, time:7861.12s\n",
            "298 epoch, D loss:0.0550, G loss:0.1076, training acc: 87.62%, time:7887.32s\n",
            "299 epoch, D loss:0.0537, G loss:0.1078, training acc: 87.59%, time:7913.57s\n",
            "300 epoch, D loss:0.0551, G loss:0.1077, training acc: 87.74%, time:7939.70s\n",
            "test acc: 66.19%, time:7942.30s\n",
            "301 epoch, D loss:0.0570, G loss:0.1065, training acc: 87.74%, time:7968.91s\n",
            "302 epoch, D loss:0.0552, G loss:0.1067, training acc: 88.98%, time:7995.14s\n",
            "303 epoch, D loss:0.0560, G loss:0.1066, training acc: 88.42%, time:8021.24s\n",
            "304 epoch, D loss:0.0566, G loss:0.1050, training acc: 88.79%, time:8047.32s\n",
            "305 epoch, D loss:0.0550, G loss:0.1057, training acc: 88.89%, time:8073.93s\n",
            "306 epoch, D loss:0.0542, G loss:0.1081, training acc: 88.45%, time:8100.11s\n",
            "307 epoch, D loss:0.0552, G loss:0.1071, training acc: 87.74%, time:8126.27s\n",
            "308 epoch, D loss:0.0611, G loss:0.1028, training acc: 87.43%, time:8152.41s\n",
            "309 epoch, D loss:0.0633, G loss:0.1039, training acc: 88.51%, time:8179.16s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "310 epoch, D loss:0.0614, G loss:0.1056, training acc: 88.24%, time:8205.35s\n",
            "test acc: 69.09%, time:8207.95s\n",
            "311 epoch, D loss:0.0624, G loss:0.1034, training acc: 88.61%, time:8234.13s\n",
            "312 epoch, D loss:0.0624, G loss:0.1036, training acc: 88.95%, time:8260.29s\n",
            "313 epoch, D loss:0.0602, G loss:0.1063, training acc: 89.01%, time:8286.91s\n",
            "314 epoch, D loss:0.0641, G loss:0.1031, training acc: 88.76%, time:8313.12s\n",
            "315 epoch, D loss:0.0618, G loss:0.1052, training acc: 88.61%, time:8339.32s\n",
            "316 epoch, D loss:0.0605, G loss:0.1044, training acc: 88.89%, time:8365.49s\n",
            "317 epoch, D loss:0.0586, G loss:0.1037, training acc: 89.04%, time:8392.13s\n",
            "318 epoch, D loss:0.0595, G loss:0.1055, training acc: 88.20%, time:8418.34s\n",
            "319 epoch, D loss:0.0574, G loss:0.1071, training acc: 88.64%, time:8444.53s\n",
            "320 epoch, D loss:0.0597, G loss:0.1043, training acc: 88.54%, time:8470.90s\n",
            "test acc: 68.14%, time:8473.49s\n",
            "321 epoch, D loss:0.0613, G loss:0.1051, training acc: 87.86%, time:8500.15s\n",
            "322 epoch, D loss:0.0576, G loss:0.1063, training acc: 88.98%, time:8526.33s\n",
            "323 epoch, D loss:0.0541, G loss:0.1087, training acc: 88.64%, time:8552.49s\n",
            "324 epoch, D loss:0.0556, G loss:0.1071, training acc: 89.13%, time:8578.75s\n",
            "325 epoch, D loss:0.0566, G loss:0.1072, training acc: 87.96%, time:8605.45s\n",
            "326 epoch, D loss:0.0612, G loss:0.1032, training acc: 89.20%, time:8631.62s\n",
            "327 epoch, D loss:0.0661, G loss:0.1027, training acc: 90.40%, time:8657.39s\n",
            "328 epoch, D loss:0.0613, G loss:0.1059, training acc: 92.23%, time:8683.35s\n",
            "329 epoch, D loss:0.0568, G loss:0.1074, training acc: 92.11%, time:8709.88s\n",
            "330 epoch, D loss:0.0647, G loss:0.0999, training acc: 92.54%, time:8735.81s\n",
            "test acc: 70.51%, time:8738.36s\n",
            "331 epoch, D loss:0.0640, G loss:0.1008, training acc: 92.69%, time:8764.28s\n",
            "332 epoch, D loss:0.0564, G loss:0.1046, training acc: 92.41%, time:8790.09s\n",
            "333 epoch, D loss:0.0546, G loss:0.1065, training acc: 91.27%, time:8816.23s\n",
            "334 epoch, D loss:0.0584, G loss:0.1045, training acc: 91.61%, time:8842.04s\n",
            "335 epoch, D loss:0.0598, G loss:0.1054, training acc: 91.98%, time:8867.93s\n",
            "336 epoch, D loss:0.0620, G loss:0.1056, training acc: 89.88%, time:8893.62s\n",
            "337 epoch, D loss:0.0667, G loss:0.1005, training acc: 92.94%, time:8920.07s\n",
            "338 epoch, D loss:0.0647, G loss:0.1010, training acc: 91.76%, time:8945.99s\n",
            "339 epoch, D loss:0.0655, G loss:0.0996, training acc: 93.96%, time:8971.79s\n",
            "340 epoch, D loss:0.0696, G loss:0.0980, training acc: 94.15%, time:8997.50s\n",
            "test acc: 72.63%, time:9000.20s\n",
            "341 epoch, D loss:0.0654, G loss:0.1001, training acc: 94.02%, time:9026.56s\n",
            "342 epoch, D loss:0.0600, G loss:0.1026, training acc: 94.98%, time:9052.33s\n",
            "343 epoch, D loss:0.0618, G loss:0.1032, training acc: 94.43%, time:9078.14s\n",
            "344 epoch, D loss:0.0558, G loss:0.1049, training acc: 95.39%, time:9104.16s\n",
            "345 epoch, D loss:0.0614, G loss:0.0997, training acc: 93.50%, time:9130.78s\n",
            "346 epoch, D loss:0.0588, G loss:0.1008, training acc: 95.29%, time:9156.68s\n",
            "347 epoch, D loss:0.0558, G loss:0.1039, training acc: 95.51%, time:9182.62s\n",
            "348 epoch, D loss:0.0570, G loss:0.1041, training acc: 93.28%, time:9208.62s\n",
            "349 epoch, D loss:0.0570, G loss:0.1035, training acc: 95.17%, time:9235.14s\n",
            "350 epoch, D loss:0.0558, G loss:0.1037, training acc: 94.89%, time:9261.24s\n",
            "test acc: 68.53%, time:9263.81s\n",
            "351 epoch, D loss:0.0557, G loss:0.1042, training acc: 94.24%, time:9289.88s\n",
            "352 epoch, D loss:0.0556, G loss:0.1041, training acc: 95.05%, time:9315.78s\n",
            "353 epoch, D loss:0.0557, G loss:0.1041, training acc: 95.63%, time:9342.31s\n",
            "354 epoch, D loss:0.0578, G loss:0.1038, training acc: 95.14%, time:9368.23s\n",
            "355 epoch, D loss:0.0523, G loss:0.1084, training acc: 88.02%, time:9394.22s\n",
            "356 epoch, D loss:0.0543, G loss:0.1046, training acc: 92.60%, time:9420.18s\n",
            "357 epoch, D loss:0.0569, G loss:0.1029, training acc: 93.84%, time:9446.64s\n",
            "358 epoch, D loss:0.0546, G loss:0.1048, training acc: 95.02%, time:9472.81s\n",
            "359 epoch, D loss:0.0560, G loss:0.1050, training acc: 94.40%, time:9498.90s\n",
            "360 epoch, D loss:0.0566, G loss:0.1037, training acc: 95.54%, time:9524.85s\n",
            "test acc: 64.35%, time:9527.44s\n",
            "361 epoch, D loss:0.0555, G loss:0.1045, training acc: 95.48%, time:9553.95s\n",
            "362 epoch, D loss:0.0544, G loss:0.1056, training acc: 95.70%, time:9580.07s\n",
            "363 epoch, D loss:0.0537, G loss:0.1036, training acc: 95.39%, time:9606.01s\n",
            "364 epoch, D loss:0.0546, G loss:0.1034, training acc: 95.20%, time:9632.06s\n",
            "365 epoch, D loss:0.0556, G loss:0.1030, training acc: 94.98%, time:9658.53s\n",
            "366 epoch, D loss:0.0547, G loss:0.1062, training acc: 95.08%, time:9684.55s\n",
            "367 epoch, D loss:0.0534, G loss:0.1079, training acc: 94.74%, time:9710.44s\n",
            "368 epoch, D loss:0.0534, G loss:0.1054, training acc: 95.33%, time:9736.38s\n",
            "369 epoch, D loss:0.0570, G loss:0.1038, training acc: 90.99%, time:9763.14s\n",
            "370 epoch, D loss:0.0587, G loss:0.1034, training acc: 93.78%, time:9789.27s\n",
            "test acc: 68.30%, time:9791.85s\n",
            "371 epoch, D loss:0.0547, G loss:0.1081, training acc: 95.08%, time:9817.94s\n",
            "372 epoch, D loss:0.0530, G loss:0.1095, training acc: 94.98%, time:9844.06s\n",
            "373 epoch, D loss:0.0520, G loss:0.1059, training acc: 94.92%, time:9870.81s\n",
            "374 epoch, D loss:0.0552, G loss:0.1015, training acc: 95.20%, time:9896.85s\n",
            "375 epoch, D loss:0.0549, G loss:0.1034, training acc: 95.54%, time:9922.92s\n",
            "376 epoch, D loss:0.0544, G loss:0.1073, training acc: 95.26%, time:9949.09s\n",
            "377 epoch, D loss:0.0524, G loss:0.1073, training acc: 94.98%, time:9975.81s\n",
            "378 epoch, D loss:0.0513, G loss:0.1088, training acc: 95.05%, time:10001.94s\n",
            "379 epoch, D loss:0.0529, G loss:0.1072, training acc: 95.08%, time:10028.26s\n",
            "380 epoch, D loss:0.0542, G loss:0.1054, training acc: 95.54%, time:10054.35s\n",
            "test acc: 68.58%, time:10056.94s\n",
            "381 epoch, D loss:0.0564, G loss:0.1040, training acc: 95.48%, time:10083.67s\n",
            "382 epoch, D loss:0.0552, G loss:0.1070, training acc: 96.10%, time:10109.83s\n",
            "383 epoch, D loss:0.0506, G loss:0.1099, training acc: 95.98%, time:10136.13s\n",
            "384 epoch, D loss:0.0566, G loss:0.1036, training acc: 95.17%, time:10162.43s\n",
            "385 epoch, D loss:0.0598, G loss:0.1005, training acc: 94.74%, time:10189.28s\n",
            "386 epoch, D loss:0.0555, G loss:0.1052, training acc: 95.39%, time:10215.38s\n",
            "387 epoch, D loss:0.0544, G loss:0.1097, training acc: 84.89%, time:10241.71s\n",
            "388 epoch, D loss:0.0551, G loss:0.1054, training acc: 88.95%, time:10268.09s\n",
            "389 epoch, D loss:0.0564, G loss:0.1032, training acc: 94.06%, time:10294.95s\n",
            "390 epoch, D loss:0.0554, G loss:0.1064, training acc: 93.81%, time:10321.27s\n",
            "test acc: 67.28%, time:10323.86s\n",
            "391 epoch, D loss:0.0569, G loss:0.1035, training acc: 94.06%, time:10350.18s\n",
            "392 epoch, D loss:0.0568, G loss:0.1037, training acc: 95.02%, time:10376.55s\n",
            "393 epoch, D loss:0.0591, G loss:0.1039, training acc: 95.82%, time:10403.44s\n",
            "394 epoch, D loss:0.0592, G loss:0.1046, training acc: 95.20%, time:10429.89s\n",
            "395 epoch, D loss:0.0599, G loss:0.1033, training acc: 95.94%, time:10456.43s\n",
            "396 epoch, D loss:0.0597, G loss:0.1013, training acc: 96.07%, time:10483.02s\n",
            "397 epoch, D loss:0.0575, G loss:0.1014, training acc: 96.10%, time:10510.20s\n",
            "398 epoch, D loss:0.0594, G loss:0.1001, training acc: 94.43%, time:10536.50s\n",
            "399 epoch, D loss:0.0595, G loss:0.1029, training acc: 95.17%, time:10562.87s\n",
            "400 epoch, D loss:0.0603, G loss:0.1029, training acc: 94.40%, time:10589.30s\n",
            "test acc: 67.70%, time:10591.88s\n",
            "401 epoch, D loss:0.0673, G loss:0.1002, training acc: 94.55%, time:10618.69s\n",
            "402 epoch, D loss:0.0625, G loss:0.1021, training acc: 95.70%, time:10645.07s\n",
            "403 epoch, D loss:0.0635, G loss:0.1003, training acc: 95.79%, time:10671.63s\n",
            "404 epoch, D loss:0.0657, G loss:0.0990, training acc: 96.16%, time:10697.99s\n",
            "405 epoch, D loss:0.0605, G loss:0.1027, training acc: 96.07%, time:10724.84s\n",
            "406 epoch, D loss:0.0591, G loss:0.1029, training acc: 96.04%, time:10751.15s\n",
            "407 epoch, D loss:0.0620, G loss:0.1010, training acc: 96.25%, time:10777.49s\n",
            "408 epoch, D loss:0.0611, G loss:0.1027, training acc: 96.10%, time:10803.72s\n",
            "409 epoch, D loss:0.0639, G loss:0.1015, training acc: 95.94%, time:10830.63s\n",
            "410 epoch, D loss:0.0624, G loss:0.1028, training acc: 96.25%, time:10856.97s\n",
            "test acc: 71.42%, time:10859.56s\n",
            "411 epoch, D loss:0.0585, G loss:0.1065, training acc: 96.13%, time:10885.91s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "412 epoch, D loss:0.0566, G loss:0.1079, training acc: 96.25%, time:10912.29s\n",
            "413 epoch, D loss:0.0574, G loss:0.1053, training acc: 96.28%, time:10939.03s\n",
            "414 epoch, D loss:0.0594, G loss:0.1029, training acc: 96.22%, time:10965.26s\n",
            "415 epoch, D loss:0.0584, G loss:0.1029, training acc: 96.25%, time:10991.58s\n",
            "416 epoch, D loss:0.0591, G loss:0.1035, training acc: 96.16%, time:11017.87s\n",
            "417 epoch, D loss:0.0604, G loss:0.1021, training acc: 96.25%, time:11044.61s\n",
            "418 epoch, D loss:0.0636, G loss:0.1000, training acc: 96.25%, time:11070.90s\n",
            "419 epoch, D loss:0.0625, G loss:0.1004, training acc: 96.16%, time:11097.34s\n",
            "420 epoch, D loss:0.0580, G loss:0.1039, training acc: 96.22%, time:11123.67s\n",
            "test acc: 71.49%, time:11126.26s\n",
            "421 epoch, D loss:0.0559, G loss:0.1073, training acc: 96.22%, time:11153.09s\n",
            "422 epoch, D loss:0.0543, G loss:0.1073, training acc: 96.25%, time:11179.34s\n",
            "423 epoch, D loss:0.0548, G loss:0.1055, training acc: 96.01%, time:11205.74s\n",
            "424 epoch, D loss:0.0568, G loss:0.1043, training acc: 96.22%, time:11232.02s\n",
            "425 epoch, D loss:0.0559, G loss:0.1055, training acc: 96.22%, time:11258.90s\n",
            "426 epoch, D loss:0.0530, G loss:0.1085, training acc: 96.19%, time:11285.27s\n",
            "427 epoch, D loss:0.0537, G loss:0.1084, training acc: 96.04%, time:11311.65s\n",
            "428 epoch, D loss:0.0521, G loss:0.1086, training acc: 95.85%, time:11337.84s\n",
            "429 epoch, D loss:0.0545, G loss:0.1047, training acc: 94.64%, time:11364.74s\n",
            "430 epoch, D loss:0.0532, G loss:0.1032, training acc: 95.98%, time:11390.96s\n",
            "test acc: 71.40%, time:11393.55s\n",
            "431 epoch, D loss:0.0536, G loss:0.1039, training acc: 95.85%, time:11419.73s\n",
            "432 epoch, D loss:0.0505, G loss:0.1073, training acc: 96.10%, time:11446.14s\n",
            "433 epoch, D loss:0.0526, G loss:0.1059, training acc: 96.28%, time:11473.01s\n",
            "434 epoch, D loss:0.0567, G loss:0.1027, training acc: 96.04%, time:11499.56s\n",
            "435 epoch, D loss:0.0599, G loss:0.1023, training acc: 96.13%, time:11525.72s\n",
            "436 epoch, D loss:0.0615, G loss:0.1002, training acc: 96.22%, time:11551.73s\n",
            "437 epoch, D loss:0.0609, G loss:0.1019, training acc: 95.88%, time:11578.41s\n",
            "438 epoch, D loss:0.0620, G loss:0.1020, training acc: 96.19%, time:11604.44s\n",
            "439 epoch, D loss:0.0621, G loss:0.0999, training acc: 95.85%, time:11630.42s\n",
            "440 epoch, D loss:0.0650, G loss:0.0992, training acc: 95.48%, time:11656.61s\n",
            "test acc: 71.28%, time:11659.16s\n",
            "441 epoch, D loss:0.0661, G loss:0.0976, training acc: 95.91%, time:11685.70s\n",
            "442 epoch, D loss:0.0635, G loss:0.0995, training acc: 96.19%, time:11711.79s\n",
            "443 epoch, D loss:0.0610, G loss:0.1008, training acc: 96.22%, time:11737.90s\n",
            "444 epoch, D loss:0.0652, G loss:0.0983, training acc: 96.25%, time:11764.30s\n",
            "445 epoch, D loss:0.0650, G loss:0.0989, training acc: 96.19%, time:11791.38s\n",
            "446 epoch, D loss:0.0624, G loss:0.1006, training acc: 96.16%, time:11817.80s\n",
            "447 epoch, D loss:0.0613, G loss:0.1010, training acc: 96.07%, time:11844.01s\n",
            "448 epoch, D loss:0.0622, G loss:0.1010, training acc: 95.67%, time:11870.37s\n",
            "449 epoch, D loss:0.0559, G loss:0.1049, training acc: 95.67%, time:11897.26s\n",
            "450 epoch, D loss:0.0600, G loss:0.1015, training acc: 95.51%, time:11923.46s\n",
            "test acc: 58.02%, time:11926.05s\n",
            "451 epoch, D loss:0.0615, G loss:0.1011, training acc: 96.07%, time:11952.47s\n",
            "452 epoch, D loss:0.0598, G loss:0.1043, training acc: 96.10%, time:11978.67s\n",
            "453 epoch, D loss:0.0584, G loss:0.1050, training acc: 95.94%, time:12005.53s\n",
            "454 epoch, D loss:0.0580, G loss:0.1054, training acc: 95.23%, time:12031.80s\n",
            "455 epoch, D loss:0.0550, G loss:0.1050, training acc: 95.67%, time:12058.12s\n",
            "456 epoch, D loss:0.0529, G loss:0.1055, training acc: 95.73%, time:12084.40s\n",
            "457 epoch, D loss:0.0552, G loss:0.1030, training acc: 95.94%, time:12111.28s\n",
            "458 epoch, D loss:0.0519, G loss:0.1030, training acc: 95.76%, time:12137.61s\n",
            "459 epoch, D loss:0.0549, G loss:0.0998, training acc: 94.09%, time:12163.97s\n",
            "460 epoch, D loss:0.0553, G loss:0.0996, training acc: 95.51%, time:12190.22s\n",
            "test acc: 72.35%, time:12192.80s\n",
            "461 epoch, D loss:0.0523, G loss:0.1030, training acc: 95.42%, time:12219.62s\n",
            "462 epoch, D loss:0.0517, G loss:0.1034, training acc: 95.73%, time:12245.81s\n",
            "463 epoch, D loss:0.0534, G loss:0.1020, training acc: 95.67%, time:12272.05s\n",
            "464 epoch, D loss:0.0537, G loss:0.1017, training acc: 94.95%, time:12298.32s\n",
            "465 epoch, D loss:0.0522, G loss:0.1046, training acc: 95.54%, time:12325.11s\n",
            "466 epoch, D loss:0.0506, G loss:0.1059, training acc: 95.26%, time:12351.37s\n",
            "467 epoch, D loss:0.0496, G loss:0.1074, training acc: 95.45%, time:12377.73s\n",
            "468 epoch, D loss:0.0491, G loss:0.1062, training acc: 95.29%, time:12404.10s\n",
            "469 epoch, D loss:0.0499, G loss:0.1061, training acc: 95.67%, time:12431.09s\n",
            "470 epoch, D loss:0.0510, G loss:0.1047, training acc: 95.51%, time:12457.37s\n",
            "test acc: 68.37%, time:12459.95s\n",
            "471 epoch, D loss:0.0514, G loss:0.1045, training acc: 95.11%, time:12486.23s\n",
            "472 epoch, D loss:0.0512, G loss:0.1053, training acc: 95.76%, time:12512.56s\n",
            "473 epoch, D loss:0.0510, G loss:0.1072, training acc: 95.48%, time:12539.28s\n",
            "474 epoch, D loss:0.0513, G loss:0.1066, training acc: 95.33%, time:12566.23s\n",
            "475 epoch, D loss:0.0512, G loss:0.1058, training acc: 95.48%, time:12592.70s\n",
            "476 epoch, D loss:0.0517, G loss:0.1047, training acc: 95.42%, time:12619.45s\n",
            "477 epoch, D loss:0.0519, G loss:0.1044, training acc: 95.79%, time:12646.42s\n",
            "478 epoch, D loss:0.0505, G loss:0.1065, training acc: 96.01%, time:12672.94s\n",
            "479 epoch, D loss:0.0504, G loss:0.1070, training acc: 95.76%, time:12699.25s\n",
            "480 epoch, D loss:0.0511, G loss:0.1063, training acc: 94.40%, time:12725.57s\n",
            "test acc: 69.60%, time:12728.15s\n",
            "481 epoch, D loss:0.0513, G loss:0.1070, training acc: 95.60%, time:12755.35s\n",
            "482 epoch, D loss:0.0502, G loss:0.1065, training acc: 95.73%, time:12781.53s\n",
            "483 epoch, D loss:0.0499, G loss:0.1074, training acc: 95.98%, time:12808.02s\n",
            "484 epoch, D loss:0.0491, G loss:0.1081, training acc: 95.33%, time:12834.46s\n",
            "485 epoch, D loss:0.0510, G loss:0.1058, training acc: 95.57%, time:12861.40s\n",
            "486 epoch, D loss:0.0530, G loss:0.1038, training acc: 95.67%, time:12887.69s\n",
            "487 epoch, D loss:0.0522, G loss:0.1056, training acc: 95.94%, time:12914.09s\n",
            "488 epoch, D loss:0.0493, G loss:0.1099, training acc: 95.73%, time:12940.29s\n",
            "489 epoch, D loss:0.0490, G loss:0.1073, training acc: 95.45%, time:12967.56s\n",
            "490 epoch, D loss:0.0516, G loss:0.1038, training acc: 95.88%, time:12994.16s\n",
            "test acc: 70.88%, time:12996.74s\n",
            "491 epoch, D loss:0.0526, G loss:0.1040, training acc: 95.51%, time:13023.18s\n",
            "492 epoch, D loss:0.0513, G loss:0.1064, training acc: 95.98%, time:13049.35s\n",
            "493 epoch, D loss:0.0501, G loss:0.1079, training acc: 95.94%, time:13076.25s\n",
            "494 epoch, D loss:0.0510, G loss:0.1053, training acc: 96.01%, time:13102.61s\n",
            "495 epoch, D loss:0.0526, G loss:0.1044, training acc: 95.29%, time:13128.75s\n",
            "496 epoch, D loss:0.0510, G loss:0.1056, training acc: 95.79%, time:13154.90s\n",
            "497 epoch, D loss:0.0513, G loss:0.1055, training acc: 95.76%, time:13181.65s\n",
            "498 epoch, D loss:0.0506, G loss:0.1076, training acc: 95.98%, time:13208.02s\n",
            "499 epoch, D loss:0.0512, G loss:0.1060, training acc: 96.04%, time:13234.23s\n",
            "500 epoch, D loss:0.0526, G loss:0.1048, training acc: 95.85%, time:13260.42s\n",
            "test acc: 73.12%, time:13263.01s\n",
            "501 epoch, D loss:0.0531, G loss:0.1047, training acc: 95.88%, time:13289.77s\n",
            "502 epoch, D loss:0.0532, G loss:0.1051, training acc: 95.54%, time:13315.97s\n",
            "503 epoch, D loss:0.0521, G loss:0.1077, training acc: 95.63%, time:13342.20s\n",
            "504 epoch, D loss:0.0535, G loss:0.1050, training acc: 96.07%, time:13368.55s\n",
            "505 epoch, D loss:0.0538, G loss:0.1049, training acc: 96.13%, time:13395.52s\n",
            "506 epoch, D loss:0.0519, G loss:0.1074, training acc: 95.94%, time:13422.01s\n",
            "507 epoch, D loss:0.0529, G loss:0.1075, training acc: 94.95%, time:13448.38s\n",
            "508 epoch, D loss:0.0547, G loss:0.1050, training acc: 96.01%, time:13474.53s\n",
            "509 epoch, D loss:0.0550, G loss:0.1041, training acc: 95.88%, time:13501.45s\n",
            "510 epoch, D loss:0.0515, G loss:0.1063, training acc: 96.04%, time:13527.87s\n",
            "test acc: 64.88%, time:13530.46s\n",
            "511 epoch, D loss:0.0481, G loss:0.1081, training acc: 96.04%, time:13556.62s\n",
            "512 epoch, D loss:0.0529, G loss:0.1036, training acc: 95.82%, time:13583.03s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "513 epoch, D loss:0.0549, G loss:0.1030, training acc: 96.25%, time:13609.77s\n",
            "514 epoch, D loss:0.0525, G loss:0.1060, training acc: 96.19%, time:13636.02s\n",
            "515 epoch, D loss:0.0532, G loss:0.1065, training acc: 96.04%, time:13662.39s\n",
            "516 epoch, D loss:0.0538, G loss:0.1077, training acc: 96.28%, time:13688.58s\n",
            "517 epoch, D loss:0.0555, G loss:0.1056, training acc: 96.19%, time:13715.39s\n",
            "518 epoch, D loss:0.0529, G loss:0.1072, training acc: 96.04%, time:13741.60s\n",
            "519 epoch, D loss:0.0505, G loss:0.1092, training acc: 96.25%, time:13767.84s\n",
            "520 epoch, D loss:0.0493, G loss:0.1081, training acc: 96.13%, time:13794.24s\n",
            "test acc: 69.09%, time:13796.85s\n",
            "521 epoch, D loss:0.0506, G loss:0.1066, training acc: 95.98%, time:13823.59s\n",
            "522 epoch, D loss:0.0512, G loss:0.1048, training acc: 95.98%, time:13849.75s\n",
            "523 epoch, D loss:0.0546, G loss:0.1025, training acc: 96.01%, time:13875.94s\n",
            "524 epoch, D loss:0.0564, G loss:0.1017, training acc: 96.25%, time:13902.12s\n",
            "525 epoch, D loss:0.0576, G loss:0.1014, training acc: 95.94%, time:13928.91s\n",
            "526 epoch, D loss:0.0557, G loss:0.1031, training acc: 96.25%, time:13955.15s\n",
            "527 epoch, D loss:0.0517, G loss:0.1060, training acc: 96.19%, time:13981.49s\n",
            "528 epoch, D loss:0.0526, G loss:0.1043, training acc: 96.22%, time:14007.59s\n",
            "529 epoch, D loss:0.0537, G loss:0.1034, training acc: 96.22%, time:14034.28s\n",
            "530 epoch, D loss:0.0554, G loss:0.1024, training acc: 96.16%, time:14060.37s\n",
            "test acc: 71.56%, time:14062.97s\n",
            "531 epoch, D loss:0.0551, G loss:0.1054, training acc: 95.94%, time:14089.21s\n",
            "532 epoch, D loss:0.0537, G loss:0.1060, training acc: 96.19%, time:14115.35s\n",
            "533 epoch, D loss:0.0550, G loss:0.1031, training acc: 96.01%, time:14142.13s\n",
            "534 epoch, D loss:0.0559, G loss:0.1011, training acc: 96.16%, time:14168.32s\n",
            "535 epoch, D loss:0.0567, G loss:0.1002, training acc: 95.91%, time:14194.38s\n",
            "536 epoch, D loss:0.0558, G loss:0.1009, training acc: 95.88%, time:14220.67s\n",
            "537 epoch, D loss:0.0552, G loss:0.1032, training acc: 96.19%, time:14247.32s\n",
            "538 epoch, D loss:0.0568, G loss:0.1022, training acc: 96.16%, time:14273.40s\n",
            "539 epoch, D loss:0.0596, G loss:0.1021, training acc: 96.01%, time:14299.55s\n",
            "540 epoch, D loss:0.0587, G loss:0.1041, training acc: 95.94%, time:14325.62s\n",
            "test acc: 67.65%, time:14328.22s\n",
            "541 epoch, D loss:0.0565, G loss:0.1070, training acc: 92.11%, time:14354.93s\n",
            "542 epoch, D loss:0.0552, G loss:0.1068, training acc: 93.93%, time:14380.96s\n",
            "543 epoch, D loss:0.0531, G loss:0.1068, training acc: 95.60%, time:14407.02s\n",
            "544 epoch, D loss:0.0528, G loss:0.1041, training acc: 95.98%, time:14433.18s\n",
            "545 epoch, D loss:0.0570, G loss:0.1004, training acc: 96.01%, time:14459.90s\n",
            "546 epoch, D loss:0.0549, G loss:0.1037, training acc: 95.36%, time:14486.00s\n",
            "547 epoch, D loss:0.0549, G loss:0.1038, training acc: 96.28%, time:14511.96s\n",
            "548 epoch, D loss:0.0565, G loss:0.1034, training acc: 95.88%, time:14538.04s\n",
            "549 epoch, D loss:0.0588, G loss:0.1020, training acc: 95.54%, time:14564.62s\n",
            "550 epoch, D loss:0.0574, G loss:0.1034, training acc: 96.01%, time:14590.72s\n",
            "test acc: 72.93%, time:14593.32s\n",
            "551 epoch, D loss:0.0580, G loss:0.1016, training acc: 96.25%, time:14619.74s\n",
            "552 epoch, D loss:0.0600, G loss:0.1000, training acc: 96.19%, time:14646.21s\n",
            "553 epoch, D loss:0.0619, G loss:0.0995, training acc: 96.10%, time:14672.86s\n",
            "554 epoch, D loss:0.0592, G loss:0.1014, training acc: 95.82%, time:14699.09s\n",
            "555 epoch, D loss:0.0591, G loss:0.1013, training acc: 96.16%, time:14725.73s\n",
            "556 epoch, D loss:0.0570, G loss:0.1019, training acc: 96.22%, time:14752.34s\n",
            "557 epoch, D loss:0.0551, G loss:0.1044, training acc: 95.85%, time:14779.43s\n",
            "558 epoch, D loss:0.0509, G loss:0.1067, training acc: 93.22%, time:14805.97s\n",
            "559 epoch, D loss:0.0568, G loss:0.0999, training acc: 91.55%, time:14832.13s\n",
            "560 epoch, D loss:0.0569, G loss:0.0993, training acc: 95.02%, time:14858.21s\n",
            "test acc: 66.44%, time:14860.81s\n",
            "561 epoch, D loss:0.0550, G loss:0.1026, training acc: 95.36%, time:14887.47s\n",
            "562 epoch, D loss:0.0551, G loss:0.1033, training acc: 95.98%, time:14913.58s\n",
            "563 epoch, D loss:0.0542, G loss:0.1030, training acc: 95.51%, time:14939.68s\n",
            "564 epoch, D loss:0.0517, G loss:0.1039, training acc: 95.91%, time:14965.80s\n",
            "565 epoch, D loss:0.0537, G loss:0.1023, training acc: 95.57%, time:14992.60s\n",
            "566 epoch, D loss:0.0551, G loss:0.1021, training acc: 95.63%, time:15018.62s\n",
            "567 epoch, D loss:0.0542, G loss:0.1040, training acc: 96.01%, time:15044.65s\n",
            "568 epoch, D loss:0.0540, G loss:0.1049, training acc: 95.76%, time:15070.74s\n",
            "569 epoch, D loss:0.0562, G loss:0.1034, training acc: 95.73%, time:15097.60s\n",
            "570 epoch, D loss:0.0527, G loss:0.1070, training acc: 96.07%, time:15123.83s\n",
            "test acc: 69.02%, time:15126.43s\n",
            "571 epoch, D loss:0.0522, G loss:0.1079, training acc: 96.07%, time:15152.48s\n",
            "572 epoch, D loss:0.0524, G loss:0.1050, training acc: 96.13%, time:15178.57s\n",
            "573 epoch, D loss:0.0500, G loss:0.1070, training acc: 96.19%, time:15205.29s\n",
            "574 epoch, D loss:0.0511, G loss:0.1052, training acc: 96.22%, time:15231.29s\n",
            "575 epoch, D loss:0.0560, G loss:0.1011, training acc: 95.94%, time:15257.48s\n",
            "576 epoch, D loss:0.0549, G loss:0.1030, training acc: 95.98%, time:15283.58s\n",
            "577 epoch, D loss:0.0547, G loss:0.1047, training acc: 96.13%, time:15310.21s\n",
            "578 epoch, D loss:0.0532, G loss:0.1070, training acc: 95.79%, time:15336.65s\n",
            "579 epoch, D loss:0.0526, G loss:0.1069, training acc: 96.13%, time:15362.77s\n",
            "580 epoch, D loss:0.0552, G loss:0.1038, training acc: 96.16%, time:15388.97s\n",
            "test acc: 73.16%, time:15391.56s\n",
            "581 epoch, D loss:0.0598, G loss:0.0998, training acc: 96.19%, time:15418.15s\n",
            "582 epoch, D loss:0.0595, G loss:0.1011, training acc: 95.94%, time:15444.34s\n",
            "583 epoch, D loss:0.0560, G loss:0.1044, training acc: 96.10%, time:15470.40s\n",
            "584 epoch, D loss:0.0557, G loss:0.1050, training acc: 96.04%, time:15496.49s\n",
            "585 epoch, D loss:0.0575, G loss:0.1029, training acc: 96.01%, time:15523.24s\n",
            "586 epoch, D loss:0.0554, G loss:0.1053, training acc: 96.01%, time:15549.40s\n",
            "587 epoch, D loss:0.0530, G loss:0.1090, training acc: 94.40%, time:15575.63s\n",
            "588 epoch, D loss:0.0533, G loss:0.1061, training acc: 95.33%, time:15601.81s\n",
            "589 epoch, D loss:0.0536, G loss:0.1055, training acc: 96.07%, time:15628.56s\n",
            "590 epoch, D loss:0.0537, G loss:0.1046, training acc: 95.94%, time:15654.71s\n",
            "test acc: 67.28%, time:15657.32s\n",
            "591 epoch, D loss:0.0563, G loss:0.1034, training acc: 95.29%, time:15683.42s\n",
            "592 epoch, D loss:0.0570, G loss:0.1021, training acc: 94.98%, time:15709.65s\n",
            "593 epoch, D loss:0.0568, G loss:0.1020, training acc: 95.85%, time:15736.39s\n",
            "594 epoch, D loss:0.0597, G loss:0.1009, training acc: 96.22%, time:15762.50s\n",
            "595 epoch, D loss:0.0568, G loss:0.1032, training acc: 96.04%, time:15788.71s\n",
            "596 epoch, D loss:0.0561, G loss:0.1054, training acc: 96.01%, time:15814.84s\n",
            "597 epoch, D loss:0.0574, G loss:0.1031, training acc: 94.33%, time:15841.57s\n",
            "598 epoch, D loss:0.0575, G loss:0.1031, training acc: 95.73%, time:15867.68s\n",
            "599 epoch, D loss:0.0588, G loss:0.1013, training acc: 95.98%, time:15893.77s\n",
            "600 epoch, D loss:0.0577, G loss:0.1023, training acc: 96.16%, time:15919.89s\n",
            "test acc: 73.12%, time:15922.49s\n",
            "601 epoch, D loss:0.0579, G loss:0.1016, training acc: 96.25%, time:15949.09s\n",
            "602 epoch, D loss:0.0571, G loss:0.1019, training acc: 96.22%, time:15975.14s\n",
            "603 epoch, D loss:0.0581, G loss:0.1017, training acc: 96.22%, time:16001.28s\n",
            "604 epoch, D loss:0.0593, G loss:0.1019, training acc: 96.04%, time:16027.35s\n",
            "605 epoch, D loss:0.0567, G loss:0.1028, training acc: 96.28%, time:16054.10s\n",
            "606 epoch, D loss:0.0563, G loss:0.1031, training acc: 96.25%, time:16080.16s\n",
            "607 epoch, D loss:0.0556, G loss:0.1044, training acc: 96.19%, time:16106.29s\n",
            "608 epoch, D loss:0.0525, G loss:0.1081, training acc: 96.19%, time:16132.38s\n",
            "609 epoch, D loss:0.0519, G loss:0.1074, training acc: 96.28%, time:16159.05s\n",
            "610 epoch, D loss:0.0546, G loss:0.1040, training acc: 96.19%, time:16184.96s\n",
            "test acc: 73.37%, time:16187.54s\n",
            "611 epoch, D loss:0.0531, G loss:0.1038, training acc: 96.28%, time:16213.41s\n",
            "612 epoch, D loss:0.0528, G loss:0.1048, training acc: 96.28%, time:16239.46s\n",
            "613 epoch, D loss:0.0557, G loss:0.1043, training acc: 96.16%, time:16265.93s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "614 epoch, D loss:0.0561, G loss:0.1034, training acc: 96.16%, time:16291.92s\n",
            "615 epoch, D loss:0.0554, G loss:0.1042, training acc: 96.28%, time:16317.97s\n",
            "616 epoch, D loss:0.0539, G loss:0.1057, training acc: 96.25%, time:16343.88s\n",
            "617 epoch, D loss:0.0554, G loss:0.1038, training acc: 96.19%, time:16370.36s\n",
            "618 epoch, D loss:0.0583, G loss:0.1023, training acc: 96.25%, time:16396.14s\n",
            "619 epoch, D loss:0.0563, G loss:0.1047, training acc: 96.22%, time:16422.09s\n",
            "620 epoch, D loss:0.0534, G loss:0.1086, training acc: 96.25%, time:16447.95s\n",
            "test acc: 73.58%, time:16450.50s\n",
            "621 epoch, D loss:0.0517, G loss:0.1101, training acc: 96.25%, time:16476.84s\n",
            "622 epoch, D loss:0.0512, G loss:0.1086, training acc: 96.28%, time:16502.90s\n",
            "623 epoch, D loss:0.0501, G loss:0.1088, training acc: 96.25%, time:16528.84s\n",
            "624 epoch, D loss:0.0530, G loss:0.1057, training acc: 96.19%, time:16554.81s\n",
            "625 epoch, D loss:0.0571, G loss:0.1023, training acc: 96.25%, time:16581.03s\n",
            "626 epoch, D loss:0.0577, G loss:0.1022, training acc: 96.22%, time:16606.81s\n",
            "627 epoch, D loss:0.0597, G loss:0.1016, training acc: 96.04%, time:16632.80s\n",
            "628 epoch, D loss:0.0560, G loss:0.1046, training acc: 96.22%, time:16658.85s\n",
            "629 epoch, D loss:0.0536, G loss:0.1062, training acc: 96.22%, time:16685.31s\n",
            "630 epoch, D loss:0.0529, G loss:0.1065, training acc: 95.98%, time:16711.15s\n",
            "test acc: 74.60%, time:16713.70s\n",
            "631 epoch, D loss:0.0520, G loss:0.1057, training acc: 96.19%, time:16739.66s\n",
            "632 epoch, D loss:0.0556, G loss:0.1029, training acc: 95.94%, time:16765.68s\n",
            "633 epoch, D loss:0.0558, G loss:0.1042, training acc: 96.10%, time:16792.20s\n",
            "634 epoch, D loss:0.0516, G loss:0.1082, training acc: 96.22%, time:16818.18s\n",
            "635 epoch, D loss:0.0540, G loss:0.1054, training acc: 95.45%, time:16844.17s\n",
            "636 epoch, D loss:0.0630, G loss:0.0990, training acc: 96.01%, time:16870.21s\n",
            "637 epoch, D loss:0.0610, G loss:0.1015, training acc: 96.22%, time:16896.74s\n",
            "638 epoch, D loss:0.0554, G loss:0.1059, training acc: 96.25%, time:16922.87s\n",
            "639 epoch, D loss:0.0510, G loss:0.1108, training acc: 95.70%, time:16948.86s\n",
            "640 epoch, D loss:0.0521, G loss:0.1068, training acc: 96.19%, time:16975.03s\n",
            "test acc: 74.95%, time:16977.59s\n",
            "641 epoch, D loss:0.0517, G loss:0.1059, training acc: 95.98%, time:17004.07s\n",
            "642 epoch, D loss:0.0574, G loss:0.1013, training acc: 96.25%, time:17030.09s\n",
            "643 epoch, D loss:0.0585, G loss:0.1012, training acc: 96.16%, time:17056.14s\n",
            "644 epoch, D loss:0.0551, G loss:0.1045, training acc: 96.19%, time:17081.93s\n",
            "645 epoch, D loss:0.0549, G loss:0.1050, training acc: 96.28%, time:17108.40s\n",
            "646 epoch, D loss:0.0558, G loss:0.1057, training acc: 96.28%, time:17134.45s\n",
            "647 epoch, D loss:0.0542, G loss:0.1052, training acc: 96.28%, time:17160.45s\n",
            "648 epoch, D loss:0.0547, G loss:0.1044, training acc: 96.28%, time:17186.45s\n",
            "649 epoch, D loss:0.0581, G loss:0.1024, training acc: 96.28%, time:17212.89s\n",
            "650 epoch, D loss:0.0545, G loss:0.1048, training acc: 96.16%, time:17238.81s\n",
            "test acc: 74.19%, time:17241.47s\n",
            "651 epoch, D loss:0.0532, G loss:0.1059, training acc: 96.22%, time:17267.29s\n",
            "652 epoch, D loss:0.0531, G loss:0.1062, training acc: 96.22%, time:17293.21s\n",
            "653 epoch, D loss:0.0540, G loss:0.1055, training acc: 96.28%, time:17319.61s\n",
            "654 epoch, D loss:0.0540, G loss:0.1050, training acc: 96.28%, time:17345.59s\n",
            "655 epoch, D loss:0.0530, G loss:0.1046, training acc: 96.22%, time:17371.45s\n",
            "656 epoch, D loss:0.0549, G loss:0.1027, training acc: 96.25%, time:17397.34s\n",
            "657 epoch, D loss:0.0579, G loss:0.1017, training acc: 96.22%, time:17423.80s\n",
            "658 epoch, D loss:0.0563, G loss:0.1029, training acc: 96.25%, time:17449.73s\n",
            "659 epoch, D loss:0.0528, G loss:0.1068, training acc: 95.98%, time:17475.73s\n",
            "660 epoch, D loss:0.0525, G loss:0.1068, training acc: 96.28%, time:17501.76s\n",
            "test acc: 74.30%, time:17504.42s\n",
            "661 epoch, D loss:0.0551, G loss:0.1040, training acc: 96.28%, time:17530.85s\n",
            "662 epoch, D loss:0.0569, G loss:0.1045, training acc: 95.85%, time:17556.78s\n",
            "663 epoch, D loss:0.0548, G loss:0.1058, training acc: 95.98%, time:17582.75s\n",
            "664 epoch, D loss:0.0545, G loss:0.1063, training acc: 96.13%, time:17608.81s\n",
            "665 epoch, D loss:0.0545, G loss:0.1060, training acc: 96.28%, time:17635.25s\n",
            "666 epoch, D loss:0.0561, G loss:0.1037, training acc: 96.28%, time:17661.14s\n",
            "667 epoch, D loss:0.0578, G loss:0.1021, training acc: 96.28%, time:17687.31s\n",
            "668 epoch, D loss:0.0602, G loss:0.1004, training acc: 96.22%, time:17713.39s\n",
            "669 epoch, D loss:0.0566, G loss:0.1046, training acc: 96.28%, time:17739.74s\n",
            "670 epoch, D loss:0.0543, G loss:0.1074, training acc: 96.22%, time:17765.74s\n",
            "test acc: 73.72%, time:17768.40s\n",
            "671 epoch, D loss:0.0566, G loss:0.1049, training acc: 96.28%, time:17794.46s\n",
            "672 epoch, D loss:0.0561, G loss:0.1052, training acc: 96.28%, time:17820.43s\n",
            "673 epoch, D loss:0.0535, G loss:0.1071, training acc: 96.25%, time:17846.87s\n",
            "674 epoch, D loss:0.0554, G loss:0.1033, training acc: 96.28%, time:17872.84s\n",
            "675 epoch, D loss:0.0567, G loss:0.1045, training acc: 96.28%, time:17898.86s\n",
            "676 epoch, D loss:0.0576, G loss:0.1023, training acc: 96.28%, time:17924.82s\n",
            "677 epoch, D loss:0.0584, G loss:0.1022, training acc: 96.28%, time:17951.39s\n",
            "678 epoch, D loss:0.0570, G loss:0.1033, training acc: 96.25%, time:17977.25s\n",
            "679 epoch, D loss:0.0578, G loss:0.1031, training acc: 96.19%, time:18003.25s\n",
            "680 epoch, D loss:0.0570, G loss:0.1025, training acc: 95.98%, time:18029.26s\n",
            "test acc: 75.35%, time:18031.85s\n",
            "681 epoch, D loss:0.0552, G loss:0.1041, training acc: 96.07%, time:18058.40s\n",
            "682 epoch, D loss:0.0566, G loss:0.1023, training acc: 95.20%, time:18084.45s\n",
            "683 epoch, D loss:0.0537, G loss:0.1034, training acc: 96.25%, time:18110.31s\n",
            "684 epoch, D loss:0.0527, G loss:0.1044, training acc: 96.19%, time:18136.16s\n",
            "685 epoch, D loss:0.0539, G loss:0.1032, training acc: 95.70%, time:18162.71s\n",
            "686 epoch, D loss:0.0568, G loss:0.1022, training acc: 96.28%, time:18188.57s\n",
            "687 epoch, D loss:0.0618, G loss:0.0984, training acc: 96.25%, time:18214.50s\n",
            "688 epoch, D loss:0.0601, G loss:0.1003, training acc: 96.19%, time:18240.35s\n",
            "689 epoch, D loss:0.0552, G loss:0.1050, training acc: 96.13%, time:18266.79s\n",
            "690 epoch, D loss:0.0535, G loss:0.1065, training acc: 96.25%, time:18292.80s\n",
            "test acc: 76.33%, time:18295.35s\n",
            "691 epoch, D loss:0.0592, G loss:0.1019, training acc: 96.28%, time:18321.35s\n",
            "692 epoch, D loss:0.0635, G loss:0.0996, training acc: 96.28%, time:18347.40s\n",
            "693 epoch, D loss:0.0595, G loss:0.1025, training acc: 95.94%, time:18373.94s\n",
            "694 epoch, D loss:0.0561, G loss:0.1046, training acc: 96.10%, time:18399.76s\n",
            "695 epoch, D loss:0.0545, G loss:0.1047, training acc: 96.10%, time:18425.75s\n",
            "696 epoch, D loss:0.0547, G loss:0.1051, training acc: 96.25%, time:18451.70s\n",
            "697 epoch, D loss:0.0574, G loss:0.1027, training acc: 96.22%, time:18478.22s\n",
            "698 epoch, D loss:0.0581, G loss:0.1016, training acc: 96.22%, time:18504.25s\n",
            "699 epoch, D loss:0.0543, G loss:0.1066, training acc: 96.22%, time:18530.12s\n",
            "700 epoch, D loss:0.0523, G loss:0.1079, training acc: 96.22%, time:18556.03s\n",
            "test acc: 75.98%, time:18558.58s\n",
            "701 epoch, D loss:0.0551, G loss:0.1053, training acc: 96.28%, time:18585.22s\n",
            "702 epoch, D loss:0.0567, G loss:0.1029, training acc: 96.25%, time:18611.11s\n",
            "703 epoch, D loss:0.0535, G loss:0.1062, training acc: 95.39%, time:18637.06s\n",
            "704 epoch, D loss:0.0531, G loss:0.1065, training acc: 96.16%, time:18663.27s\n",
            "705 epoch, D loss:0.0519, G loss:0.1060, training acc: 96.07%, time:18689.82s\n",
            "706 epoch, D loss:0.0526, G loss:0.1042, training acc: 96.25%, time:18715.91s\n",
            "707 epoch, D loss:0.0587, G loss:0.0999, training acc: 96.28%, time:18741.89s\n",
            "708 epoch, D loss:0.0600, G loss:0.0994, training acc: 96.28%, time:18767.83s\n",
            "709 epoch, D loss:0.0564, G loss:0.1031, training acc: 96.19%, time:18794.29s\n",
            "710 epoch, D loss:0.0526, G loss:0.1058, training acc: 96.07%, time:18820.32s\n",
            "test acc: 71.70%, time:18822.87s\n",
            "711 epoch, D loss:0.0510, G loss:0.1082, training acc: 95.82%, time:18848.80s\n",
            "712 epoch, D loss:0.0523, G loss:0.1059, training acc: 96.16%, time:18875.00s\n",
            "713 epoch, D loss:0.0536, G loss:0.1046, training acc: 96.28%, time:18901.42s\n",
            "714 epoch, D loss:0.0525, G loss:0.1071, training acc: 91.73%, time:18927.41s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "715 epoch, D loss:0.0503, G loss:0.1065, training acc: 95.98%, time:18953.26s\n",
            "716 epoch, D loss:0.0525, G loss:0.1024, training acc: 96.04%, time:18979.29s\n",
            "717 epoch, D loss:0.0527, G loss:0.1015, training acc: 95.82%, time:19005.92s\n",
            "718 epoch, D loss:0.0524, G loss:0.1022, training acc: 96.10%, time:19031.84s\n",
            "719 epoch, D loss:0.0517, G loss:0.1033, training acc: 95.98%, time:19057.76s\n",
            "720 epoch, D loss:0.0519, G loss:0.1032, training acc: 95.94%, time:19083.82s\n",
            "test acc: 70.58%, time:19086.36s\n",
            "721 epoch, D loss:0.0536, G loss:0.1029, training acc: 95.94%, time:19112.93s\n",
            "722 epoch, D loss:0.0520, G loss:0.1045, training acc: 95.70%, time:19138.86s\n",
            "723 epoch, D loss:0.0515, G loss:0.1052, training acc: 95.82%, time:19164.83s\n",
            "724 epoch, D loss:0.0510, G loss:0.1052, training acc: 95.91%, time:19190.70s\n",
            "725 epoch, D loss:0.0512, G loss:0.1052, training acc: 95.57%, time:19217.36s\n",
            "726 epoch, D loss:0.0489, G loss:0.1062, training acc: 95.88%, time:19243.23s\n",
            "727 epoch, D loss:0.0491, G loss:0.1053, training acc: 95.70%, time:19269.13s\n",
            "728 epoch, D loss:0.0511, G loss:0.1026, training acc: 94.46%, time:19295.00s\n",
            "729 epoch, D loss:0.0509, G loss:0.1028, training acc: 94.86%, time:19321.58s\n",
            "730 epoch, D loss:0.0518, G loss:0.1027, training acc: 95.51%, time:19347.50s\n",
            "test acc: 71.02%, time:19350.05s\n",
            "731 epoch, D loss:0.0523, G loss:0.1029, training acc: 95.36%, time:19375.94s\n",
            "732 epoch, D loss:0.0516, G loss:0.1044, training acc: 95.36%, time:19402.37s\n",
            "733 epoch, D loss:0.0507, G loss:0.1066, training acc: 95.60%, time:19429.29s\n",
            "734 epoch, D loss:0.0501, G loss:0.1067, training acc: 95.48%, time:19455.82s\n",
            "735 epoch, D loss:0.0504, G loss:0.1054, training acc: 95.57%, time:19482.59s\n",
            "736 epoch, D loss:0.0511, G loss:0.1037, training acc: 95.67%, time:19509.14s\n",
            "737 epoch, D loss:0.0531, G loss:0.1029, training acc: 95.76%, time:19536.33s\n",
            "738 epoch, D loss:0.0517, G loss:0.1053, training acc: 95.60%, time:19562.89s\n",
            "739 epoch, D loss:0.0498, G loss:0.1065, training acc: 95.94%, time:19589.41s\n",
            "740 epoch, D loss:0.0503, G loss:0.1057, training acc: 95.67%, time:19616.25s\n",
            "test acc: 69.98%, time:19618.87s\n",
            "741 epoch, D loss:0.0504, G loss:0.1052, training acc: 95.60%, time:19645.79s\n",
            "742 epoch, D loss:0.0503, G loss:0.1054, training acc: 95.45%, time:19672.52s\n",
            "743 epoch, D loss:0.0508, G loss:0.1050, training acc: 96.01%, time:19699.07s\n",
            "744 epoch, D loss:0.0512, G loss:0.1046, training acc: 95.57%, time:19725.36s\n",
            "745 epoch, D loss:0.0516, G loss:0.1045, training acc: 95.36%, time:19752.50s\n",
            "746 epoch, D loss:0.0512, G loss:0.1054, training acc: 95.63%, time:19779.09s\n",
            "747 epoch, D loss:0.0505, G loss:0.1061, training acc: 96.13%, time:19805.53s\n",
            "748 epoch, D loss:0.0508, G loss:0.1061, training acc: 95.79%, time:19832.17s\n",
            "749 epoch, D loss:0.0507, G loss:0.1062, training acc: 96.10%, time:19859.27s\n",
            "750 epoch, D loss:0.0503, G loss:0.1061, training acc: 95.79%, time:19885.57s\n",
            "test acc: 66.72%, time:19888.16s\n",
            "751 epoch, D loss:0.0506, G loss:0.1061, training acc: 96.07%, time:19914.50s\n",
            "752 epoch, D loss:0.0512, G loss:0.1063, training acc: 95.73%, time:19940.63s\n",
            "753 epoch, D loss:0.0521, G loss:0.1058, training acc: 95.11%, time:19967.39s\n",
            "754 epoch, D loss:0.0510, G loss:0.1060, training acc: 96.13%, time:19993.65s\n",
            "755 epoch, D loss:0.0528, G loss:0.1043, training acc: 96.10%, time:20019.84s\n",
            "756 epoch, D loss:0.0528, G loss:0.1040, training acc: 95.82%, time:20046.00s\n",
            "757 epoch, D loss:0.0504, G loss:0.1056, training acc: 96.10%, time:20072.81s\n",
            "758 epoch, D loss:0.0512, G loss:0.1069, training acc: 96.16%, time:20099.10s\n",
            "759 epoch, D loss:0.0518, G loss:0.1071, training acc: 96.16%, time:20125.50s\n",
            "760 epoch, D loss:0.0504, G loss:0.1085, training acc: 96.25%, time:20152.07s\n",
            "test acc: 71.86%, time:20154.66s\n",
            "761 epoch, D loss:0.0516, G loss:0.1071, training acc: 96.01%, time:20181.52s\n",
            "762 epoch, D loss:0.0509, G loss:0.1067, training acc: 95.91%, time:20207.81s\n",
            "763 epoch, D loss:0.0541, G loss:0.1039, training acc: 96.10%, time:20234.10s\n",
            "764 epoch, D loss:0.0537, G loss:0.1044, training acc: 96.10%, time:20260.29s\n",
            "765 epoch, D loss:0.0523, G loss:0.1070, training acc: 96.25%, time:20287.01s\n",
            "766 epoch, D loss:0.0504, G loss:0.1081, training acc: 96.28%, time:20313.29s\n",
            "767 epoch, D loss:0.0491, G loss:0.1091, training acc: 96.25%, time:20339.58s\n",
            "768 epoch, D loss:0.0510, G loss:0.1055, training acc: 96.25%, time:20365.82s\n",
            "769 epoch, D loss:0.0524, G loss:0.1050, training acc: 96.19%, time:20392.41s\n",
            "770 epoch, D loss:0.0510, G loss:0.1059, training acc: 96.25%, time:20418.64s\n",
            "test acc: 71.35%, time:20421.25s\n",
            "771 epoch, D loss:0.0539, G loss:0.1036, training acc: 96.25%, time:20447.33s\n",
            "772 epoch, D loss:0.0548, G loss:0.1029, training acc: 96.22%, time:20473.41s\n",
            "773 epoch, D loss:0.0541, G loss:0.1038, training acc: 96.25%, time:20499.98s\n",
            "774 epoch, D loss:0.0550, G loss:0.1030, training acc: 96.25%, time:20526.05s\n",
            "775 epoch, D loss:0.0562, G loss:0.1030, training acc: 96.10%, time:20552.21s\n",
            "776 epoch, D loss:0.0538, G loss:0.1054, training acc: 94.43%, time:20578.54s\n",
            "777 epoch, D loss:0.0500, G loss:0.1080, training acc: 96.16%, time:20605.21s\n",
            "778 epoch, D loss:0.0492, G loss:0.1087, training acc: 95.60%, time:20631.32s\n",
            "779 epoch, D loss:0.0532, G loss:0.1036, training acc: 96.16%, time:20657.45s\n",
            "780 epoch, D loss:0.0570, G loss:0.1011, training acc: 96.22%, time:20683.47s\n",
            "test acc: 74.60%, time:20686.06s\n",
            "781 epoch, D loss:0.0565, G loss:0.1024, training acc: 96.19%, time:20712.64s\n",
            "782 epoch, D loss:0.0551, G loss:0.1038, training acc: 96.22%, time:20739.34s\n",
            "783 epoch, D loss:0.0574, G loss:0.1023, training acc: 96.28%, time:20766.01s\n",
            "784 epoch, D loss:0.0595, G loss:0.1007, training acc: 96.10%, time:20792.30s\n",
            "785 epoch, D loss:0.0586, G loss:0.1014, training acc: 96.10%, time:20818.95s\n",
            "786 epoch, D loss:0.0569, G loss:0.1027, training acc: 96.07%, time:20845.01s\n",
            "787 epoch, D loss:0.0549, G loss:0.1070, training acc: 95.70%, time:20871.09s\n",
            "788 epoch, D loss:0.0489, G loss:0.1098, training acc: 94.18%, time:20897.15s\n",
            "789 epoch, D loss:0.0496, G loss:0.1058, training acc: 95.48%, time:20923.77s\n",
            "790 epoch, D loss:0.0522, G loss:0.1012, training acc: 95.60%, time:20949.90s\n",
            "test acc: 72.98%, time:20952.50s\n",
            "791 epoch, D loss:0.0515, G loss:0.1013, training acc: 95.82%, time:20978.52s\n",
            "792 epoch, D loss:0.0505, G loss:0.1041, training acc: 95.91%, time:21004.54s\n",
            "793 epoch, D loss:0.0504, G loss:0.1045, training acc: 95.70%, time:21031.09s\n",
            "794 epoch, D loss:0.0512, G loss:0.1042, training acc: 95.02%, time:21057.15s\n",
            "795 epoch, D loss:0.0515, G loss:0.1032, training acc: 95.67%, time:21083.23s\n",
            "796 epoch, D loss:0.0518, G loss:0.1029, training acc: 95.60%, time:21109.23s\n",
            "797 epoch, D loss:0.0513, G loss:0.1036, training acc: 95.94%, time:21135.85s\n",
            "798 epoch, D loss:0.0508, G loss:0.1055, training acc: 95.73%, time:21161.91s\n",
            "799 epoch, D loss:0.0502, G loss:0.1057, training acc: 94.89%, time:21187.93s\n",
            "800 epoch, D loss:0.0505, G loss:0.1049, training acc: 94.74%, time:21213.99s\n",
            "test acc: 71.19%, time:21216.59s\n",
            "801 epoch, D loss:0.0509, G loss:0.1036, training acc: 95.79%, time:21243.22s\n",
            "802 epoch, D loss:0.0509, G loss:0.1037, training acc: 95.82%, time:21269.30s\n",
            "803 epoch, D loss:0.0505, G loss:0.1057, training acc: 95.70%, time:21295.39s\n",
            "804 epoch, D loss:0.0504, G loss:0.1056, training acc: 96.01%, time:21321.67s\n",
            "805 epoch, D loss:0.0501, G loss:0.1052, training acc: 96.13%, time:21348.31s\n",
            "806 epoch, D loss:0.0504, G loss:0.1050, training acc: 95.98%, time:21374.39s\n",
            "807 epoch, D loss:0.0510, G loss:0.1041, training acc: 96.10%, time:21400.47s\n",
            "808 epoch, D loss:0.0506, G loss:0.1049, training acc: 95.91%, time:21426.58s\n",
            "809 epoch, D loss:0.0503, G loss:0.1057, training acc: 96.07%, time:21453.28s\n",
            "810 epoch, D loss:0.0500, G loss:0.1068, training acc: 96.13%, time:21479.43s\n",
            "test acc: 70.19%, time:21482.04s\n",
            "811 epoch, D loss:0.0492, G loss:0.1077, training acc: 96.10%, time:21508.15s\n",
            "812 epoch, D loss:0.0489, G loss:0.1083, training acc: 95.76%, time:21534.23s\n",
            "813 epoch, D loss:0.0491, G loss:0.1069, training acc: 95.20%, time:21560.84s\n",
            "814 epoch, D loss:0.0497, G loss:0.1063, training acc: 95.17%, time:21586.87s\n",
            "815 epoch, D loss:0.0496, G loss:0.1059, training acc: 95.94%, time:21612.87s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "816 epoch, D loss:0.0494, G loss:0.1063, training acc: 95.73%, time:21639.03s\n",
            "817 epoch, D loss:0.0493, G loss:0.1076, training acc: 94.37%, time:21665.68s\n",
            "818 epoch, D loss:0.0495, G loss:0.1075, training acc: 95.79%, time:21691.77s\n",
            "819 epoch, D loss:0.0488, G loss:0.1067, training acc: 95.85%, time:21717.87s\n",
            "820 epoch, D loss:0.0488, G loss:0.1076, training acc: 96.16%, time:21743.89s\n",
            "test acc: 70.93%, time:21746.49s\n",
            "821 epoch, D loss:0.0484, G loss:0.1070, training acc: 96.01%, time:21773.13s\n",
            "822 epoch, D loss:0.0482, G loss:0.1074, training acc: 96.04%, time:21799.20s\n",
            "823 epoch, D loss:0.0491, G loss:0.1075, training acc: 95.73%, time:21825.35s\n",
            "824 epoch, D loss:0.0486, G loss:0.1065, training acc: 96.10%, time:21851.42s\n",
            "825 epoch, D loss:0.0491, G loss:0.1064, training acc: 96.13%, time:21878.00s\n",
            "826 epoch, D loss:0.0493, G loss:0.1073, training acc: 95.94%, time:21904.40s\n",
            "827 epoch, D loss:0.0490, G loss:0.1066, training acc: 95.94%, time:21930.58s\n",
            "828 epoch, D loss:0.0491, G loss:0.1067, training acc: 96.01%, time:21957.15s\n",
            "829 epoch, D loss:0.0493, G loss:0.1070, training acc: 95.82%, time:21983.86s\n",
            "830 epoch, D loss:0.0489, G loss:0.1069, training acc: 96.25%, time:22010.01s\n",
            "test acc: 71.65%, time:22012.61s\n",
            "831 epoch, D loss:0.0488, G loss:0.1073, training acc: 96.04%, time:22038.99s\n",
            "832 epoch, D loss:0.0488, G loss:0.1070, training acc: 96.16%, time:22065.27s\n",
            "833 epoch, D loss:0.0497, G loss:0.1059, training acc: 96.04%, time:22092.00s\n",
            "834 epoch, D loss:0.0492, G loss:0.1064, training acc: 96.04%, time:22118.21s\n",
            "835 epoch, D loss:0.0489, G loss:0.1078, training acc: 96.10%, time:22144.48s\n",
            "836 epoch, D loss:0.0486, G loss:0.1070, training acc: 96.19%, time:22170.81s\n",
            "837 epoch, D loss:0.0487, G loss:0.1071, training acc: 96.04%, time:22197.60s\n",
            "838 epoch, D loss:0.0484, G loss:0.1072, training acc: 96.07%, time:22223.73s\n",
            "839 epoch, D loss:0.0486, G loss:0.1069, training acc: 96.01%, time:22249.89s\n",
            "840 epoch, D loss:0.0482, G loss:0.1068, training acc: 95.98%, time:22276.33s\n",
            "test acc: 72.84%, time:22278.92s\n",
            "841 epoch, D loss:0.0481, G loss:0.1071, training acc: 96.10%, time:22305.63s\n",
            "842 epoch, D loss:0.0479, G loss:0.1066, training acc: 96.01%, time:22331.93s\n",
            "843 epoch, D loss:0.0483, G loss:0.1076, training acc: 95.94%, time:22357.97s\n",
            "844 epoch, D loss:0.0481, G loss:0.1061, training acc: 96.01%, time:22384.37s\n",
            "845 epoch, D loss:0.0489, G loss:0.1055, training acc: 96.04%, time:22411.16s\n",
            "846 epoch, D loss:0.0497, G loss:0.1057, training acc: 95.88%, time:22437.37s\n",
            "847 epoch, D loss:0.0497, G loss:0.1061, training acc: 95.79%, time:22463.68s\n",
            "848 epoch, D loss:0.0489, G loss:0.1069, training acc: 96.01%, time:22489.71s\n",
            "849 epoch, D loss:0.0488, G loss:0.1081, training acc: 96.13%, time:22516.40s\n",
            "850 epoch, D loss:0.0493, G loss:0.1069, training acc: 95.82%, time:22542.78s\n",
            "test acc: 70.86%, time:22545.38s\n",
            "851 epoch, D loss:0.0494, G loss:0.1060, training acc: 96.07%, time:22571.69s\n",
            "852 epoch, D loss:0.0493, G loss:0.1066, training acc: 95.91%, time:22597.83s\n",
            "853 epoch, D loss:0.0490, G loss:0.1062, training acc: 95.85%, time:22624.56s\n",
            "854 epoch, D loss:0.0495, G loss:0.1057, training acc: 95.67%, time:22650.66s\n",
            "855 epoch, D loss:0.0493, G loss:0.1049, training acc: 95.57%, time:22676.91s\n",
            "856 epoch, D loss:0.0498, G loss:0.1045, training acc: 95.73%, time:22703.17s\n",
            "857 epoch, D loss:0.0496, G loss:0.1053, training acc: 95.98%, time:22729.89s\n",
            "858 epoch, D loss:0.0502, G loss:0.1057, training acc: 95.57%, time:22756.30s\n",
            "859 epoch, D loss:0.0500, G loss:0.1055, training acc: 95.57%, time:22782.71s\n",
            "860 epoch, D loss:0.0495, G loss:0.1055, training acc: 95.91%, time:22809.04s\n",
            "test acc: 66.00%, time:22811.63s\n",
            "861 epoch, D loss:0.0500, G loss:0.1051, training acc: 95.94%, time:22838.40s\n",
            "862 epoch, D loss:0.0496, G loss:0.1061, training acc: 95.57%, time:22864.62s\n",
            "863 epoch, D loss:0.0492, G loss:0.1071, training acc: 95.54%, time:22890.86s\n",
            "864 epoch, D loss:0.0496, G loss:0.1068, training acc: 95.98%, time:22916.95s\n",
            "865 epoch, D loss:0.0495, G loss:0.1070, training acc: 96.10%, time:22943.72s\n",
            "866 epoch, D loss:0.0494, G loss:0.1068, training acc: 96.22%, time:22969.96s\n",
            "867 epoch, D loss:0.0492, G loss:0.1072, training acc: 96.10%, time:22996.20s\n",
            "868 epoch, D loss:0.0502, G loss:0.1061, training acc: 95.94%, time:23022.52s\n",
            "869 epoch, D loss:0.0505, G loss:0.1065, training acc: 96.10%, time:23049.25s\n",
            "870 epoch, D loss:0.0484, G loss:0.1092, training acc: 95.98%, time:23075.55s\n",
            "test acc: 66.42%, time:23078.15s\n",
            "871 epoch, D loss:0.0478, G loss:0.1098, training acc: 96.04%, time:23104.46s\n",
            "872 epoch, D loss:0.0498, G loss:0.1058, training acc: 96.07%, time:23130.67s\n",
            "873 epoch, D loss:0.0506, G loss:0.1049, training acc: 95.63%, time:23157.52s\n",
            "874 epoch, D loss:0.0510, G loss:0.1061, training acc: 96.25%, time:23183.54s\n",
            "875 epoch, D loss:0.0498, G loss:0.1064, training acc: 96.22%, time:23209.66s\n",
            "876 epoch, D loss:0.0486, G loss:0.1074, training acc: 96.25%, time:23235.94s\n",
            "877 epoch, D loss:0.0496, G loss:0.1053, training acc: 96.10%, time:23262.65s\n",
            "878 epoch, D loss:0.0515, G loss:0.1028, training acc: 95.23%, time:23288.95s\n",
            "879 epoch, D loss:0.0529, G loss:0.1021, training acc: 95.94%, time:23315.26s\n",
            "880 epoch, D loss:0.0502, G loss:0.1047, training acc: 97.03%, time:23341.36s\n",
            "test acc: 70.14%, time:23343.96s\n",
            "881 epoch, D loss:0.0489, G loss:0.1062, training acc: 97.59%, time:23370.80s\n",
            "882 epoch, D loss:0.0499, G loss:0.1043, training acc: 97.68%, time:23396.94s\n",
            "883 epoch, D loss:0.0494, G loss:0.1058, training acc: 97.99%, time:23423.14s\n",
            "884 epoch, D loss:0.0493, G loss:0.1068, training acc: 98.11%, time:23449.40s\n",
            "885 epoch, D loss:0.0505, G loss:0.1055, training acc: 97.65%, time:23476.18s\n",
            "886 epoch, D loss:0.0514, G loss:0.1045, training acc: 97.12%, time:23502.40s\n",
            "887 epoch, D loss:0.0524, G loss:0.1042, training acc: 98.20%, time:23528.64s\n",
            "888 epoch, D loss:0.0527, G loss:0.1042, training acc: 98.14%, time:23554.99s\n",
            "889 epoch, D loss:0.0514, G loss:0.1060, training acc: 98.39%, time:23581.72s\n",
            "890 epoch, D loss:0.0516, G loss:0.1040, training acc: 98.85%, time:23607.89s\n",
            "test acc: 66.93%, time:23610.49s\n",
            "891 epoch, D loss:0.0537, G loss:0.1023, training acc: 99.04%, time:23636.74s\n",
            "892 epoch, D loss:0.0534, G loss:0.1022, training acc: 98.61%, time:23662.96s\n",
            "893 epoch, D loss:0.0543, G loss:0.1028, training acc: 98.73%, time:23689.72s\n",
            "894 epoch, D loss:0.0534, G loss:0.1032, training acc: 99.38%, time:23716.05s\n",
            "895 epoch, D loss:0.0516, G loss:0.1039, training acc: 99.63%, time:23742.37s\n",
            "896 epoch, D loss:0.0511, G loss:0.1042, training acc: 99.57%, time:23768.55s\n",
            "897 epoch, D loss:0.0507, G loss:0.1041, training acc: 99.41%, time:23795.44s\n",
            "898 epoch, D loss:0.0523, G loss:0.1030, training acc: 99.04%, time:23821.66s\n",
            "899 epoch, D loss:0.0543, G loss:0.1018, training acc: 99.44%, time:23847.95s\n",
            "900 epoch, D loss:0.0516, G loss:0.1044, training acc: 99.35%, time:23874.06s\n",
            "test acc: 61.72%, time:23876.66s\n",
            "901 epoch, D loss:0.0512, G loss:0.1045, training acc: 99.69%, time:23903.62s\n",
            "902 epoch, D loss:0.0502, G loss:0.1052, training acc: 99.75%, time:23929.77s\n",
            "903 epoch, D loss:0.0496, G loss:0.1065, training acc: 99.85%, time:23955.94s\n",
            "904 epoch, D loss:0.0498, G loss:0.1057, training acc: 99.91%, time:23982.14s\n",
            "905 epoch, D loss:0.0514, G loss:0.1039, training acc: 99.72%, time:24008.92s\n",
            "906 epoch, D loss:0.0527, G loss:0.1030, training acc: 99.81%, time:24035.06s\n",
            "907 epoch, D loss:0.0557, G loss:0.1005, training acc: 99.69%, time:24061.34s\n",
            "908 epoch, D loss:0.0557, G loss:0.1011, training acc: 99.88%, time:24087.53s\n",
            "909 epoch, D loss:0.0559, G loss:0.1018, training acc: 99.60%, time:24114.36s\n",
            "910 epoch, D loss:0.0556, G loss:0.1019, training acc: 99.78%, time:24140.59s\n",
            "test acc: 70.93%, time:24143.19s\n",
            "911 epoch, D loss:0.0537, G loss:0.1026, training acc: 99.85%, time:24169.43s\n",
            "912 epoch, D loss:0.0540, G loss:0.1024, training acc: 99.04%, time:24195.68s\n",
            "913 epoch, D loss:0.0547, G loss:0.1021, training acc: 99.66%, time:24222.32s\n",
            "914 epoch, D loss:0.0532, G loss:0.1032, training acc: 99.69%, time:24248.65s\n",
            "915 epoch, D loss:0.0502, G loss:0.1051, training acc: 99.60%, time:24274.86s\n",
            "916 epoch, D loss:0.0485, G loss:0.1057, training acc: 99.75%, time:24301.02s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "917 epoch, D loss:0.0511, G loss:0.1029, training acc: 99.60%, time:24327.91s\n",
            "918 epoch, D loss:0.0527, G loss:0.1013, training acc: 99.85%, time:24354.06s\n",
            "919 epoch, D loss:0.0526, G loss:0.1028, training acc: 99.78%, time:24380.17s\n",
            "920 epoch, D loss:0.0520, G loss:0.1037, training acc: 99.54%, time:24406.40s\n",
            "test acc: 67.51%, time:24409.00s\n",
            "921 epoch, D loss:0.0536, G loss:0.1027, training acc: 99.94%, time:24435.92s\n",
            "922 epoch, D loss:0.0537, G loss:0.1026, training acc: 99.94%, time:24462.25s\n",
            "923 epoch, D loss:0.0531, G loss:0.1031, training acc: 99.88%, time:24488.58s\n",
            "924 epoch, D loss:0.0536, G loss:0.1023, training acc: 99.88%, time:24514.86s\n",
            "925 epoch, D loss:0.0545, G loss:0.1016, training acc: 99.88%, time:24541.57s\n",
            "926 epoch, D loss:0.0556, G loss:0.1018, training acc: 99.81%, time:24567.91s\n",
            "927 epoch, D loss:0.0547, G loss:0.1033, training acc: 99.81%, time:24594.15s\n",
            "928 epoch, D loss:0.0555, G loss:0.1025, training acc: 99.91%, time:24620.42s\n",
            "929 epoch, D loss:0.0567, G loss:0.1018, training acc: 99.91%, time:24647.23s\n",
            "930 epoch, D loss:0.0569, G loss:0.1034, training acc: 99.50%, time:24673.56s\n",
            "test acc: 77.09%, time:24676.17s\n",
            "931 epoch, D loss:0.0575, G loss:0.1017, training acc: 99.81%, time:24702.47s\n",
            "932 epoch, D loss:0.0577, G loss:0.1018, training acc: 99.78%, time:24728.80s\n",
            "933 epoch, D loss:0.0574, G loss:0.1014, training acc: 99.97%, time:24755.66s\n",
            "934 epoch, D loss:0.0558, G loss:0.1016, training acc: 99.88%, time:24781.90s\n",
            "935 epoch, D loss:0.0592, G loss:0.0995, training acc: 99.81%, time:24808.08s\n",
            "936 epoch, D loss:0.0586, G loss:0.0993, training acc: 99.97%, time:24834.28s\n",
            "937 epoch, D loss:0.0573, G loss:0.1004, training acc: 99.97%, time:24861.13s\n",
            "938 epoch, D loss:0.0573, G loss:0.1007, training acc: 99.47%, time:24887.28s\n",
            "939 epoch, D loss:0.0549, G loss:0.1016, training acc: 99.44%, time:24913.55s\n",
            "940 epoch, D loss:0.0546, G loss:0.1020, training acc: 98.95%, time:24939.76s\n",
            "test acc: 68.49%, time:24942.36s\n",
            "941 epoch, D loss:0.0550, G loss:0.1017, training acc: 99.81%, time:24969.21s\n",
            "942 epoch, D loss:0.0554, G loss:0.1016, training acc: 99.94%, time:24995.48s\n",
            "943 epoch, D loss:0.0539, G loss:0.1029, training acc: 99.75%, time:25021.86s\n",
            "944 epoch, D loss:0.0550, G loss:0.1020, training acc: 99.91%, time:25048.22s\n",
            "945 epoch, D loss:0.0572, G loss:0.1002, training acc: 99.97%, time:25075.04s\n",
            "946 epoch, D loss:0.0576, G loss:0.1002, training acc: 99.94%, time:25101.34s\n",
            "947 epoch, D loss:0.0576, G loss:0.1002, training acc: 99.94%, time:25127.54s\n",
            "948 epoch, D loss:0.0570, G loss:0.1003, training acc: 99.97%, time:25153.70s\n",
            "949 epoch, D loss:0.0559, G loss:0.1012, training acc: 99.91%, time:25180.41s\n",
            "950 epoch, D loss:0.0560, G loss:0.1009, training acc: 99.94%, time:25206.64s\n",
            "test acc: 69.70%, time:25209.24s\n",
            "951 epoch, D loss:0.0553, G loss:0.1013, training acc: 99.81%, time:25235.49s\n",
            "952 epoch, D loss:0.0549, G loss:0.1021, training acc:100.00%, time:25261.74s\n",
            "953 epoch, D loss:0.0536, G loss:0.1034, training acc: 99.88%, time:25288.42s\n",
            "954 epoch, D loss:0.0535, G loss:0.1029, training acc: 99.91%, time:25314.71s\n",
            "955 epoch, D loss:0.0540, G loss:0.1020, training acc: 99.94%, time:25340.91s\n",
            "956 epoch, D loss:0.0524, G loss:0.1027, training acc: 99.85%, time:25367.19s\n",
            "957 epoch, D loss:0.0523, G loss:0.1037, training acc: 99.91%, time:25394.02s\n",
            "958 epoch, D loss:0.0528, G loss:0.1043, training acc: 99.78%, time:25420.29s\n",
            "959 epoch, D loss:0.0539, G loss:0.1036, training acc: 99.94%, time:25446.43s\n",
            "960 epoch, D loss:0.0540, G loss:0.1038, training acc: 99.94%, time:25472.63s\n",
            "test acc: 69.47%, time:25475.23s\n",
            "961 epoch, D loss:0.0524, G loss:0.1059, training acc: 99.75%, time:25501.96s\n",
            "962 epoch, D loss:0.0511, G loss:0.1067, training acc: 99.10%, time:25528.11s\n",
            "963 epoch, D loss:0.0533, G loss:0.1044, training acc: 99.88%, time:25554.34s\n",
            "964 epoch, D loss:0.0522, G loss:0.1054, training acc:100.00%, time:25580.45s\n",
            "965 epoch, D loss:0.0503, G loss:0.1065, training acc:100.00%, time:25607.20s\n",
            "966 epoch, D loss:0.0485, G loss:0.1072, training acc: 99.97%, time:25633.51s\n",
            "967 epoch, D loss:0.0511, G loss:0.1041, training acc: 99.94%, time:25659.77s\n",
            "968 epoch, D loss:0.0515, G loss:0.1042, training acc:100.00%, time:25685.91s\n",
            "969 epoch, D loss:0.0497, G loss:0.1074, training acc: 99.85%, time:25712.65s\n",
            "970 epoch, D loss:0.0502, G loss:0.1080, training acc: 99.97%, time:25738.86s\n",
            "test acc: 74.28%, time:25741.46s\n",
            "971 epoch, D loss:0.0506, G loss:0.1071, training acc: 99.78%, time:25767.80s\n",
            "972 epoch, D loss:0.0514, G loss:0.1055, training acc: 99.94%, time:25794.07s\n",
            "973 epoch, D loss:0.0522, G loss:0.1044, training acc:100.00%, time:25820.77s\n",
            "974 epoch, D loss:0.0533, G loss:0.1027, training acc:100.00%, time:25846.98s\n",
            "975 epoch, D loss:0.0534, G loss:0.1024, training acc: 99.81%, time:25873.22s\n",
            "976 epoch, D loss:0.0535, G loss:0.1028, training acc: 99.88%, time:25899.38s\n",
            "977 epoch, D loss:0.0538, G loss:0.1031, training acc: 99.78%, time:25926.20s\n",
            "978 epoch, D loss:0.0522, G loss:0.1041, training acc: 99.97%, time:25952.51s\n",
            "979 epoch, D loss:0.0545, G loss:0.1026, training acc: 99.85%, time:25978.67s\n",
            "980 epoch, D loss:0.0546, G loss:0.1028, training acc: 99.97%, time:26004.91s\n",
            "test acc: 73.65%, time:26007.51s\n",
            "981 epoch, D loss:0.0518, G loss:0.1060, training acc: 99.88%, time:26034.30s\n",
            "982 epoch, D loss:0.0502, G loss:0.1081, training acc: 99.91%, time:26060.54s\n",
            "983 epoch, D loss:0.0521, G loss:0.1064, training acc: 99.85%, time:26086.72s\n",
            "984 epoch, D loss:0.0544, G loss:0.1026, training acc: 99.41%, time:26112.93s\n",
            "985 epoch, D loss:0.0537, G loss:0.1033, training acc: 99.26%, time:26139.81s\n",
            "986 epoch, D loss:0.0523, G loss:0.1031, training acc: 99.88%, time:26166.13s\n",
            "987 epoch, D loss:0.0525, G loss:0.1025, training acc: 99.88%, time:26192.41s\n",
            "988 epoch, D loss:0.0528, G loss:0.1017, training acc: 99.97%, time:26218.56s\n",
            "989 epoch, D loss:0.0528, G loss:0.1026, training acc: 99.88%, time:26245.29s\n",
            "990 epoch, D loss:0.0500, G loss:0.1052, training acc: 99.78%, time:26271.47s\n",
            "test acc: 72.88%, time:26274.07s\n",
            "991 epoch, D loss:0.0493, G loss:0.1062, training acc: 99.85%, time:26300.26s\n",
            "992 epoch, D loss:0.0516, G loss:0.1045, training acc: 99.94%, time:26326.51s\n",
            "993 epoch, D loss:0.0526, G loss:0.1027, training acc: 99.91%, time:26353.40s\n",
            "994 epoch, D loss:0.0545, G loss:0.1016, training acc: 99.94%, time:26379.64s\n",
            "995 epoch, D loss:0.0556, G loss:0.1020, training acc:100.00%, time:26405.90s\n",
            "996 epoch, D loss:0.0590, G loss:0.1008, training acc: 99.04%, time:26432.10s\n",
            "997 epoch, D loss:0.0594, G loss:0.1000, training acc: 99.63%, time:26458.75s\n",
            "998 epoch, D loss:0.0549, G loss:0.1026, training acc: 99.23%, time:26485.02s\n",
            "999 epoch, D loss:0.0513, G loss:0.1047, training acc: 99.72%, time:26511.23s\n",
            "1000 epoch, D loss:0.0530, G loss:0.1020, training acc: 99.91%, time:26537.38s\n",
            "test acc: 74.23%, time:26539.98s\n",
            "1001 epoch, D loss:0.0575, G loss:0.0984, training acc: 99.78%, time:26566.73s\n",
            "1002 epoch, D loss:0.0590, G loss:0.0982, training acc: 99.97%, time:26593.01s\n",
            "1003 epoch, D loss:0.0564, G loss:0.1014, training acc:100.00%, time:26619.16s\n",
            "1004 epoch, D loss:0.0537, G loss:0.1039, training acc: 99.97%, time:26645.35s\n",
            "1005 epoch, D loss:0.0552, G loss:0.1049, training acc: 99.50%, time:26672.21s\n",
            "1006 epoch, D loss:0.0534, G loss:0.1057, training acc: 99.94%, time:26698.45s\n",
            "1007 epoch, D loss:0.0509, G loss:0.1074, training acc: 99.94%, time:26724.60s\n",
            "1008 epoch, D loss:0.0510, G loss:0.1071, training acc: 99.94%, time:26750.90s\n",
            "1009 epoch, D loss:0.0505, G loss:0.1070, training acc: 99.97%, time:26777.70s\n",
            "1010 epoch, D loss:0.0507, G loss:0.1071, training acc: 99.97%, time:26803.95s\n",
            "test acc: 72.49%, time:26806.55s\n",
            "1011 epoch, D loss:0.0502, G loss:0.1063, training acc: 99.94%, time:26832.77s\n",
            "1012 epoch, D loss:0.0498, G loss:0.1059, training acc: 99.97%, time:26858.95s\n",
            "1013 epoch, D loss:0.0503, G loss:0.1055, training acc: 99.91%, time:26885.87s\n",
            "1014 epoch, D loss:0.0503, G loss:0.1045, training acc: 99.91%, time:26912.09s\n",
            "1015 epoch, D loss:0.0515, G loss:0.1039, training acc: 99.88%, time:26938.38s\n",
            "1016 epoch, D loss:0.0520, G loss:0.1035, training acc: 99.94%, time:26964.72s\n",
            "1017 epoch, D loss:0.0524, G loss:0.1035, training acc: 99.97%, time:26991.57s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1018 epoch, D loss:0.0536, G loss:0.1023, training acc:100.00%, time:27017.83s\n",
            "1019 epoch, D loss:0.0558, G loss:0.1009, training acc: 99.97%, time:27044.04s\n",
            "1020 epoch, D loss:0.0566, G loss:0.1011, training acc: 99.91%, time:27070.25s\n",
            "test acc: 76.51%, time:27072.85s\n",
            "1021 epoch, D loss:0.0536, G loss:0.1043, training acc:100.00%, time:27099.52s\n",
            "1022 epoch, D loss:0.0525, G loss:0.1055, training acc: 99.94%, time:27125.72s\n",
            "1023 epoch, D loss:0.0522, G loss:0.1053, training acc:100.00%, time:27151.86s\n",
            "1024 epoch, D loss:0.0522, G loss:0.1052, training acc:100.00%, time:27178.27s\n",
            "1025 epoch, D loss:0.0521, G loss:0.1052, training acc: 99.94%, time:27205.17s\n",
            "1026 epoch, D loss:0.0509, G loss:0.1065, training acc:100.00%, time:27231.46s\n",
            "1027 epoch, D loss:0.0509, G loss:0.1070, training acc: 99.97%, time:27257.65s\n",
            "1028 epoch, D loss:0.0521, G loss:0.1053, training acc:100.00%, time:27283.74s\n",
            "1029 epoch, D loss:0.0538, G loss:0.1033, training acc: 99.78%, time:27310.48s\n",
            "1030 epoch, D loss:0.0531, G loss:0.1033, training acc:100.00%, time:27336.76s\n",
            "test acc: 77.26%, time:27339.36s\n",
            "1031 epoch, D loss:0.0516, G loss:0.1045, training acc: 99.97%, time:27365.56s\n",
            "1032 epoch, D loss:0.0520, G loss:0.1036, training acc: 99.97%, time:27391.61s\n",
            "1033 epoch, D loss:0.0534, G loss:0.1021, training acc: 99.97%, time:27418.38s\n",
            "1034 epoch, D loss:0.0545, G loss:0.1021, training acc: 99.91%, time:27444.74s\n",
            "1035 epoch, D loss:0.0528, G loss:0.1030, training acc: 99.57%, time:27470.97s\n",
            "1036 epoch, D loss:0.0513, G loss:0.1042, training acc: 99.88%, time:27497.28s\n",
            "1037 epoch, D loss:0.0519, G loss:0.1034, training acc: 99.91%, time:27524.03s\n",
            "1038 epoch, D loss:0.0524, G loss:0.1035, training acc: 99.97%, time:27550.23s\n",
            "1039 epoch, D loss:0.0509, G loss:0.1058, training acc:100.00%, time:27576.45s\n",
            "1040 epoch, D loss:0.0501, G loss:0.1075, training acc: 99.85%, time:27602.69s\n",
            "test acc: 71.70%, time:27605.29s\n",
            "1041 epoch, D loss:0.0507, G loss:0.1069, training acc: 99.85%, time:27631.98s\n",
            "1042 epoch, D loss:0.0519, G loss:0.1042, training acc: 99.69%, time:27658.28s\n",
            "1043 epoch, D loss:0.0529, G loss:0.1029, training acc: 99.97%, time:27684.46s\n",
            "1044 epoch, D loss:0.0520, G loss:0.1030, training acc: 99.94%, time:27710.71s\n",
            "1045 epoch, D loss:0.0521, G loss:0.1027, training acc: 99.88%, time:27737.49s\n",
            "1046 epoch, D loss:0.0526, G loss:0.1024, training acc: 99.94%, time:27763.74s\n",
            "1047 epoch, D loss:0.0550, G loss:0.1003, training acc: 99.91%, time:27789.91s\n",
            "1048 epoch, D loss:0.0554, G loss:0.1007, training acc: 99.97%, time:27816.19s\n",
            "1049 epoch, D loss:0.0524, G loss:0.1044, training acc:100.00%, time:27842.53s\n",
            "1050 epoch, D loss:0.0496, G loss:0.1066, training acc: 99.94%, time:27868.41s\n",
            "test acc: 72.19%, time:27870.96s\n",
            "1051 epoch, D loss:0.0513, G loss:0.1051, training acc: 99.97%, time:27896.74s\n",
            "1052 epoch, D loss:0.0541, G loss:0.1024, training acc: 99.94%, time:27922.58s\n",
            "1053 epoch, D loss:0.0532, G loss:0.1037, training acc: 99.94%, time:27948.85s\n",
            "1054 epoch, D loss:0.0513, G loss:0.1057, training acc:100.00%, time:27974.63s\n",
            "1055 epoch, D loss:0.0505, G loss:0.1059, training acc: 99.97%, time:28000.58s\n",
            "1056 epoch, D loss:0.0517, G loss:0.1044, training acc: 99.94%, time:28026.40s\n",
            "1057 epoch, D loss:0.0519, G loss:0.1042, training acc: 99.97%, time:28052.72s\n",
            "1058 epoch, D loss:0.0522, G loss:0.1043, training acc: 99.97%, time:28078.48s\n",
            "1059 epoch, D loss:0.0532, G loss:0.1044, training acc: 99.91%, time:28104.21s\n",
            "1060 epoch, D loss:0.0524, G loss:0.1052, training acc: 99.85%, time:28130.04s\n",
            "test acc: 72.47%, time:28132.60s\n",
            "1061 epoch, D loss:0.0512, G loss:0.1058, training acc:100.00%, time:28158.98s\n",
            "1062 epoch, D loss:0.0496, G loss:0.1065, training acc: 99.97%, time:28184.75s\n",
            "1063 epoch, D loss:0.0499, G loss:0.1052, training acc:100.00%, time:28210.57s\n",
            "1064 epoch, D loss:0.0523, G loss:0.1028, training acc: 99.94%, time:28236.37s\n",
            "1065 epoch, D loss:0.0532, G loss:0.1023, training acc: 99.94%, time:28262.61s\n",
            "1066 epoch, D loss:0.0522, G loss:0.1039, training acc: 99.72%, time:28288.44s\n",
            "1067 epoch, D loss:0.0523, G loss:0.1036, training acc: 99.97%, time:28314.26s\n",
            "1068 epoch, D loss:0.0513, G loss:0.1045, training acc: 99.66%, time:28340.16s\n",
            "1069 epoch, D loss:0.0523, G loss:0.1044, training acc: 99.81%, time:28366.53s\n",
            "1070 epoch, D loss:0.0527, G loss:0.1040, training acc:100.00%, time:28392.57s\n",
            "test acc: 74.63%, time:28395.12s\n",
            "1071 epoch, D loss:0.0525, G loss:0.1044, training acc: 99.97%, time:28421.03s\n",
            "1072 epoch, D loss:0.0511, G loss:0.1064, training acc: 99.91%, time:28446.86s\n",
            "1073 epoch, D loss:0.0501, G loss:0.1071, training acc: 99.78%, time:28473.14s\n",
            "1074 epoch, D loss:0.0498, G loss:0.1062, training acc: 99.78%, time:28498.96s\n",
            "1075 epoch, D loss:0.0503, G loss:0.1058, training acc: 99.97%, time:28524.68s\n",
            "1076 epoch, D loss:0.0499, G loss:0.1064, training acc: 99.94%, time:28550.53s\n",
            "1077 epoch, D loss:0.0496, G loss:0.1075, training acc: 99.97%, time:28576.85s\n",
            "1078 epoch, D loss:0.0489, G loss:0.1084, training acc: 99.97%, time:28602.72s\n",
            "1079 epoch, D loss:0.0494, G loss:0.1079, training acc: 99.97%, time:28628.53s\n",
            "1080 epoch, D loss:0.0507, G loss:0.1062, training acc:100.00%, time:28654.39s\n",
            "test acc: 74.79%, time:28656.95s\n",
            "1081 epoch, D loss:0.0539, G loss:0.1032, training acc: 99.88%, time:28683.36s\n",
            "1082 epoch, D loss:0.0552, G loss:0.1021, training acc: 99.88%, time:28709.38s\n",
            "1083 epoch, D loss:0.0535, G loss:0.1031, training acc:100.00%, time:28735.18s\n",
            "1084 epoch, D loss:0.0526, G loss:0.1044, training acc: 99.97%, time:28761.05s\n",
            "1085 epoch, D loss:0.0535, G loss:0.1030, training acc: 99.94%, time:28787.41s\n",
            "1086 epoch, D loss:0.0535, G loss:0.1026, training acc:100.00%, time:28813.30s\n",
            "1087 epoch, D loss:0.0529, G loss:0.1031, training acc:100.00%, time:28839.23s\n",
            "1088 epoch, D loss:0.0538, G loss:0.1030, training acc:100.00%, time:28865.12s\n",
            "1089 epoch, D loss:0.0531, G loss:0.1035, training acc:100.00%, time:28891.34s\n",
            "1090 epoch, D loss:0.0516, G loss:0.1052, training acc:100.00%, time:28917.13s\n",
            "test acc: 76.26%, time:28919.68s\n",
            "1091 epoch, D loss:0.0503, G loss:0.1069, training acc:100.00%, time:28945.55s\n",
            "1092 epoch, D loss:0.0506, G loss:0.1061, training acc: 99.97%, time:28971.29s\n",
            "1093 epoch, D loss:0.0522, G loss:0.1037, training acc:100.00%, time:28997.62s\n",
            "1094 epoch, D loss:0.0546, G loss:0.1026, training acc: 99.97%, time:29023.49s\n",
            "1095 epoch, D loss:0.0531, G loss:0.1038, training acc: 99.97%, time:29049.33s\n",
            "1096 epoch, D loss:0.0541, G loss:0.1032, training acc: 99.85%, time:29075.37s\n",
            "1097 epoch, D loss:0.0549, G loss:0.1026, training acc: 99.94%, time:29101.76s\n",
            "1098 epoch, D loss:0.0545, G loss:0.1032, training acc: 99.85%, time:29127.67s\n",
            "1099 epoch, D loss:0.0555, G loss:0.1030, training acc:100.00%, time:29153.55s\n",
            "1100 epoch, D loss:0.0540, G loss:0.1037, training acc: 99.94%, time:29179.39s\n",
            "test acc: 75.05%, time:29181.94s\n",
            "1101 epoch, D loss:0.0525, G loss:0.1048, training acc:100.00%, time:29208.24s\n",
            "1102 epoch, D loss:0.0532, G loss:0.1042, training acc: 99.97%, time:29234.11s\n",
            "1103 epoch, D loss:0.0522, G loss:0.1041, training acc: 99.97%, time:29259.94s\n",
            "1104 epoch, D loss:0.0506, G loss:0.1055, training acc:100.00%, time:29285.73s\n",
            "1105 epoch, D loss:0.0493, G loss:0.1071, training acc:100.00%, time:29312.21s\n",
            "1106 epoch, D loss:0.0497, G loss:0.1064, training acc: 99.91%, time:29338.02s\n",
            "1107 epoch, D loss:0.0524, G loss:0.1035, training acc: 99.44%, time:29363.75s\n",
            "1108 epoch, D loss:0.0529, G loss:0.1026, training acc: 99.94%, time:29389.46s\n",
            "1109 epoch, D loss:0.0527, G loss:0.1029, training acc:100.00%, time:29415.97s\n",
            "1110 epoch, D loss:0.0521, G loss:0.1036, training acc: 99.94%, time:29441.85s\n",
            "test acc: 77.05%, time:29444.40s\n",
            "1111 epoch, D loss:0.0524, G loss:0.1030, training acc: 99.91%, time:29470.26s\n",
            "1112 epoch, D loss:0.0521, G loss:0.1029, training acc:100.00%, time:29496.44s\n",
            "1113 epoch, D loss:0.0523, G loss:0.1029, training acc:100.00%, time:29523.32s\n",
            "1114 epoch, D loss:0.0511, G loss:0.1044, training acc: 99.88%, time:29549.45s\n",
            "1115 epoch, D loss:0.0489, G loss:0.1053, training acc: 99.94%, time:29575.61s\n",
            "1116 epoch, D loss:0.0489, G loss:0.1052, training acc:100.00%, time:29601.86s\n",
            "1117 epoch, D loss:0.0524, G loss:0.1013, training acc:100.00%, time:29628.72s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1118 epoch, D loss:0.0543, G loss:0.1004, training acc: 99.94%, time:29654.92s\n",
            "1119 epoch, D loss:0.0538, G loss:0.1007, training acc: 99.94%, time:29681.17s\n",
            "1120 epoch, D loss:0.0563, G loss:0.0993, training acc:100.00%, time:29707.35s\n",
            "test acc: 77.93%, time:29709.95s\n",
            "1121 epoch, D loss:0.0560, G loss:0.0991, training acc:100.00%, time:29736.80s\n",
            "1122 epoch, D loss:0.0553, G loss:0.0996, training acc: 99.78%, time:29762.98s\n",
            "1123 epoch, D loss:0.0565, G loss:0.0982, training acc: 99.94%, time:29789.04s\n",
            "1124 epoch, D loss:0.0552, G loss:0.0988, training acc: 99.91%, time:29815.21s\n",
            "1125 epoch, D loss:0.0537, G loss:0.1003, training acc: 99.97%, time:29842.01s\n",
            "1126 epoch, D loss:0.0520, G loss:0.1021, training acc: 99.97%, time:29868.16s\n",
            "1127 epoch, D loss:0.0515, G loss:0.1032, training acc: 99.41%, time:29894.35s\n",
            "1128 epoch, D loss:0.0503, G loss:0.1041, training acc: 99.97%, time:29920.52s\n",
            "1129 epoch, D loss:0.0523, G loss:0.1047, training acc: 99.35%, time:29947.23s\n",
            "1130 epoch, D loss:0.0530, G loss:0.1028, training acc: 99.47%, time:29973.44s\n",
            "test acc: 79.86%, time:29976.04s\n",
            "1131 epoch, D loss:0.0545, G loss:0.1020, training acc: 99.91%, time:30002.28s\n",
            "1132 epoch, D loss:0.0546, G loss:0.1024, training acc: 99.91%, time:30028.51s\n",
            "1133 epoch, D loss:0.0561, G loss:0.1016, training acc:100.00%, time:30055.22s\n",
            "1134 epoch, D loss:0.0573, G loss:0.1006, training acc: 99.75%, time:30081.52s\n",
            "1135 epoch, D loss:0.0547, G loss:0.1025, training acc: 99.91%, time:30107.64s\n",
            "1136 epoch, D loss:0.0507, G loss:0.1059, training acc: 99.88%, time:30133.78s\n",
            "1137 epoch, D loss:0.0495, G loss:0.1064, training acc: 99.88%, time:30160.56s\n",
            "1138 epoch, D loss:0.0494, G loss:0.1054, training acc:100.00%, time:30186.82s\n",
            "1139 epoch, D loss:0.0496, G loss:0.1054, training acc: 99.75%, time:30212.95s\n",
            "1140 epoch, D loss:0.0511, G loss:0.1048, training acc: 99.94%, time:30239.18s\n",
            "test acc: 76.91%, time:30241.78s\n",
            "1141 epoch, D loss:0.0502, G loss:0.1052, training acc: 99.91%, time:30268.54s\n",
            "1142 epoch, D loss:0.0510, G loss:0.1053, training acc: 99.94%, time:30294.68s\n",
            "1143 epoch, D loss:0.0516, G loss:0.1052, training acc:100.00%, time:30321.09s\n",
            "1144 epoch, D loss:0.0512, G loss:0.1053, training acc: 99.97%, time:30347.41s\n",
            "1145 epoch, D loss:0.0507, G loss:0.1057, training acc:100.00%, time:30374.31s\n",
            "1146 epoch, D loss:0.0497, G loss:0.1064, training acc: 99.78%, time:30400.47s\n",
            "1147 epoch, D loss:0.0494, G loss:0.1067, training acc: 98.27%, time:30426.75s\n",
            "1148 epoch, D loss:0.0494, G loss:0.1054, training acc: 99.41%, time:30452.99s\n",
            "1149 epoch, D loss:0.0520, G loss:0.1033, training acc: 98.76%, time:30479.62s\n",
            "1150 epoch, D loss:0.0520, G loss:0.1030, training acc: 99.91%, time:30505.99s\n",
            "test acc: 79.05%, time:30508.59s\n",
            "1151 epoch, D loss:0.0504, G loss:0.1047, training acc: 99.91%, time:30534.87s\n",
            "1152 epoch, D loss:0.0492, G loss:0.1068, training acc: 99.57%, time:30561.07s\n",
            "1153 epoch, D loss:0.0483, G loss:0.1076, training acc: 99.91%, time:30587.82s\n",
            "1154 epoch, D loss:0.0465, G loss:0.1075, training acc: 99.97%, time:30614.05s\n",
            "1155 epoch, D loss:0.0469, G loss:0.1072, training acc: 99.94%, time:30640.21s\n",
            "1156 epoch, D loss:0.0516, G loss:0.1032, training acc: 99.97%, time:30666.66s\n",
            "1157 epoch, D loss:0.0545, G loss:0.1005, training acc: 99.91%, time:30693.59s\n",
            "1158 epoch, D loss:0.0563, G loss:0.1002, training acc: 99.94%, time:30719.98s\n",
            "1159 epoch, D loss:0.0545, G loss:0.1018, training acc:100.00%, time:30746.40s\n",
            "1160 epoch, D loss:0.0515, G loss:0.1034, training acc: 99.97%, time:30772.87s\n",
            "test acc: 78.05%, time:30775.47s\n",
            "1161 epoch, D loss:0.0500, G loss:0.1049, training acc: 99.97%, time:30802.51s\n",
            "1162 epoch, D loss:0.0510, G loss:0.1039, training acc:100.00%, time:30828.97s\n",
            "1163 epoch, D loss:0.0529, G loss:0.1024, training acc:100.00%, time:30855.52s\n",
            "1164 epoch, D loss:0.0528, G loss:0.1035, training acc: 99.94%, time:30881.87s\n",
            "1165 epoch, D loss:0.0517, G loss:0.1052, training acc: 99.91%, time:30908.80s\n",
            "1166 epoch, D loss:0.0518, G loss:0.1055, training acc: 99.85%, time:30935.29s\n",
            "1167 epoch, D loss:0.0521, G loss:0.1056, training acc: 98.89%, time:30961.81s\n",
            "1168 epoch, D loss:0.0520, G loss:0.1044, training acc: 99.23%, time:30988.34s\n",
            "1169 epoch, D loss:0.0520, G loss:0.1038, training acc: 99.88%, time:31015.30s\n",
            "1170 epoch, D loss:0.0527, G loss:0.1026, training acc: 99.97%, time:31041.86s\n",
            "test acc: 78.79%, time:31044.46s\n",
            "1171 epoch, D loss:0.0536, G loss:0.1019, training acc: 99.94%, time:31070.82s\n",
            "1172 epoch, D loss:0.0537, G loss:0.1020, training acc:100.00%, time:31097.26s\n",
            "1173 epoch, D loss:0.0523, G loss:0.1031, training acc: 99.97%, time:31124.35s\n",
            "1174 epoch, D loss:0.0513, G loss:0.1038, training acc:100.00%, time:31150.85s\n",
            "1175 epoch, D loss:0.0518, G loss:0.1033, training acc: 99.97%, time:31177.31s\n",
            "1176 epoch, D loss:0.0554, G loss:0.1009, training acc:100.00%, time:31203.54s\n",
            "1177 epoch, D loss:0.0573, G loss:0.1001, training acc:100.00%, time:31230.55s\n",
            "1178 epoch, D loss:0.0558, G loss:0.1011, training acc:100.00%, time:31256.94s\n",
            "1179 epoch, D loss:0.0544, G loss:0.1027, training acc:100.00%, time:31283.31s\n",
            "1180 epoch, D loss:0.0533, G loss:0.1036, training acc: 99.97%, time:31309.69s\n",
            "test acc: 77.02%, time:31312.29s\n",
            "1181 epoch, D loss:0.0524, G loss:0.1045, training acc: 99.97%, time:31339.18s\n",
            "1182 epoch, D loss:0.0514, G loss:0.1052, training acc: 99.97%, time:31365.56s\n",
            "1183 epoch, D loss:0.0511, G loss:0.1056, training acc:100.00%, time:31391.95s\n",
            "1184 epoch, D loss:0.0516, G loss:0.1050, training acc:100.00%, time:31418.45s\n",
            "1185 epoch, D loss:0.0512, G loss:0.1050, training acc: 99.94%, time:31445.43s\n",
            "1186 epoch, D loss:0.0502, G loss:0.1060, training acc: 99.97%, time:31471.88s\n",
            "1187 epoch, D loss:0.0513, G loss:0.1042, training acc:100.00%, time:31498.18s\n",
            "1188 epoch, D loss:0.0522, G loss:0.1031, training acc: 99.97%, time:31524.45s\n",
            "1189 epoch, D loss:0.0521, G loss:0.1037, training acc:100.00%, time:31551.47s\n",
            "1190 epoch, D loss:0.0527, G loss:0.1034, training acc:100.00%, time:31577.82s\n",
            "test acc: 76.77%, time:31580.42s\n",
            "1191 epoch, D loss:0.0557, G loss:0.1012, training acc:100.00%, time:31607.18s\n",
            "1192 epoch, D loss:0.0569, G loss:0.1007, training acc: 99.97%, time:31633.67s\n",
            "1193 epoch, D loss:0.0542, G loss:0.1026, training acc: 99.97%, time:31660.86s\n",
            "1194 epoch, D loss:0.0513, G loss:0.1054, training acc: 99.94%, time:31687.25s\n",
            "1195 epoch, D loss:0.0511, G loss:0.1056, training acc: 99.97%, time:31713.71s\n",
            "1196 epoch, D loss:0.0522, G loss:0.1044, training acc: 99.94%, time:31740.25s\n",
            "1197 epoch, D loss:0.0548, G loss:0.1017, training acc: 99.97%, time:31766.94s\n",
            "1198 epoch, D loss:0.0571, G loss:0.1000, training acc: 99.97%, time:31793.10s\n",
            "1199 epoch, D loss:0.0537, G loss:0.1024, training acc: 99.94%, time:31819.26s\n",
            "1200 epoch, D loss:0.0496, G loss:0.1067, training acc: 99.29%, time:31845.34s\n",
            "test acc: 75.26%, time:31847.93s\n",
            "1201 epoch, D loss:0.0502, G loss:0.1038, training acc: 99.91%, time:31874.55s\n",
            "1202 epoch, D loss:0.0518, G loss:0.1029, training acc: 99.97%, time:31900.74s\n",
            "1203 epoch, D loss:0.0516, G loss:0.1048, training acc: 99.97%, time:31926.93s\n",
            "1204 epoch, D loss:0.0508, G loss:0.1057, training acc:100.00%, time:31953.02s\n",
            "1205 epoch, D loss:0.0523, G loss:0.1040, training acc:100.00%, time:31979.84s\n",
            "1206 epoch, D loss:0.0526, G loss:0.1036, training acc:100.00%, time:32005.99s\n",
            "1207 epoch, D loss:0.0500, G loss:0.1051, training acc:100.00%, time:32032.25s\n",
            "1208 epoch, D loss:0.0485, G loss:0.1060, training acc:100.00%, time:32058.46s\n",
            "1209 epoch, D loss:0.0473, G loss:0.1065, training acc: 99.75%, time:32085.15s\n",
            "1210 epoch, D loss:0.0481, G loss:0.1051, training acc: 98.79%, time:32111.38s\n",
            "test acc: 76.51%, time:32113.98s\n",
            "1211 epoch, D loss:0.0503, G loss:0.1020, training acc: 99.94%, time:32140.15s\n",
            "1212 epoch, D loss:0.0521, G loss:0.1012, training acc: 99.94%, time:32166.26s\n",
            "1213 epoch, D loss:0.0525, G loss:0.1012, training acc: 99.97%, time:32193.04s\n",
            "1214 epoch, D loss:0.0511, G loss:0.1031, training acc: 99.81%, time:32219.18s\n",
            "1215 epoch, D loss:0.0520, G loss:0.1030, training acc:100.00%, time:32245.33s\n",
            "1216 epoch, D loss:0.0542, G loss:0.1016, training acc: 99.94%, time:32271.51s\n",
            "1217 epoch, D loss:0.0546, G loss:0.1010, training acc: 99.88%, time:32298.29s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1218 epoch, D loss:0.0531, G loss:0.1033, training acc: 99.94%, time:32324.40s\n",
            "1219 epoch, D loss:0.0500, G loss:0.1049, training acc: 99.85%, time:32350.57s\n",
            "1220 epoch, D loss:0.0490, G loss:0.1052, training acc: 99.91%, time:32376.69s\n",
            "test acc: 74.09%, time:32379.29s\n",
            "1221 epoch, D loss:0.0500, G loss:0.1031, training acc: 99.94%, time:32406.07s\n",
            "1222 epoch, D loss:0.0510, G loss:0.1022, training acc:100.00%, time:32432.40s\n",
            "1223 epoch, D loss:0.0521, G loss:0.1014, training acc:100.00%, time:32458.62s\n",
            "1224 epoch, D loss:0.0516, G loss:0.1018, training acc: 99.97%, time:32484.68s\n",
            "1225 epoch, D loss:0.0511, G loss:0.1030, training acc: 99.75%, time:32511.45s\n",
            "1226 epoch, D loss:0.0496, G loss:0.1038, training acc: 99.97%, time:32537.65s\n",
            "1227 epoch, D loss:0.0503, G loss:0.1032, training acc: 99.94%, time:32563.87s\n",
            "1228 epoch, D loss:0.0528, G loss:0.1018, training acc: 99.91%, time:32590.14s\n",
            "1229 epoch, D loss:0.0533, G loss:0.1014, training acc: 99.91%, time:32616.96s\n",
            "1230 epoch, D loss:0.0545, G loss:0.1017, training acc: 99.88%, time:32643.18s\n",
            "test acc: 71.44%, time:32645.75s\n",
            "1231 epoch, D loss:0.0527, G loss:0.1034, training acc:100.00%, time:32671.78s\n",
            "1232 epoch, D loss:0.0512, G loss:0.1044, training acc: 99.94%, time:32697.77s\n",
            "1233 epoch, D loss:0.0525, G loss:0.1030, training acc: 99.97%, time:32724.43s\n",
            "1234 epoch, D loss:0.0524, G loss:0.1025, training acc: 99.91%, time:32750.51s\n",
            "1235 epoch, D loss:0.0505, G loss:0.1037, training acc: 99.97%, time:32776.67s\n",
            "1236 epoch, D loss:0.0506, G loss:0.1032, training acc:100.00%, time:32802.86s\n",
            "1237 epoch, D loss:0.0499, G loss:0.1034, training acc: 99.85%, time:32829.51s\n",
            "1238 epoch, D loss:0.0490, G loss:0.1041, training acc:100.00%, time:32855.48s\n",
            "1239 epoch, D loss:0.0489, G loss:0.1050, training acc: 99.63%, time:32881.68s\n",
            "1240 epoch, D loss:0.0487, G loss:0.1057, training acc: 99.63%, time:32907.74s\n",
            "test acc: 75.09%, time:32910.36s\n",
            "1241 epoch, D loss:0.0500, G loss:0.1045, training acc: 99.85%, time:32937.01s\n",
            "1242 epoch, D loss:0.0523, G loss:0.1055, training acc: 98.95%, time:32963.13s\n",
            "1243 epoch, D loss:0.0534, G loss:0.1032, training acc: 96.13%, time:32989.09s\n",
            "1244 epoch, D loss:0.0538, G loss:0.1028, training acc: 99.72%, time:33015.20s\n",
            "1245 epoch, D loss:0.0528, G loss:0.1033, training acc: 99.81%, time:33042.02s\n",
            "1246 epoch, D loss:0.0528, G loss:0.1037, training acc: 99.94%, time:33068.01s\n",
            "1247 epoch, D loss:0.0538, G loss:0.1033, training acc: 99.91%, time:33094.01s\n",
            "1248 epoch, D loss:0.0527, G loss:0.1039, training acc: 99.47%, time:33120.08s\n",
            "1249 epoch, D loss:0.0516, G loss:0.1052, training acc: 98.24%, time:33146.71s\n",
            "1250 epoch, D loss:0.0508, G loss:0.1054, training acc: 98.24%, time:33172.77s\n",
            "test acc: 76.60%, time:33175.35s\n",
            "1251 epoch, D loss:0.0505, G loss:0.1056, training acc: 99.35%, time:33201.34s\n",
            "1252 epoch, D loss:0.0496, G loss:0.1056, training acc: 99.88%, time:33227.38s\n",
            "1253 epoch, D loss:0.0490, G loss:0.1060, training acc: 99.47%, time:33253.92s\n",
            "1254 epoch, D loss:0.0478, G loss:0.1064, training acc: 99.88%, time:33279.98s\n",
            "1255 epoch, D loss:0.0487, G loss:0.1050, training acc: 99.69%, time:33306.01s\n",
            "1256 epoch, D loss:0.0490, G loss:0.1045, training acc: 99.63%, time:33331.87s\n",
            "1257 epoch, D loss:0.0495, G loss:0.1038, training acc: 99.85%, time:33358.39s\n",
            "1258 epoch, D loss:0.0498, G loss:0.1036, training acc: 99.41%, time:33384.32s\n",
            "1259 epoch, D loss:0.0500, G loss:0.1031, training acc: 99.38%, time:33410.37s\n",
            "1260 epoch, D loss:0.0508, G loss:0.1027, training acc: 99.75%, time:33436.46s\n",
            "test acc: 74.23%, time:33439.03s\n",
            "1261 epoch, D loss:0.0505, G loss:0.1034, training acc: 99.60%, time:33465.59s\n",
            "1262 epoch, D loss:0.0499, G loss:0.1032, training acc: 99.81%, time:33491.58s\n",
            "1263 epoch, D loss:0.0496, G loss:0.1036, training acc: 99.88%, time:33517.66s\n",
            "1264 epoch, D loss:0.0493, G loss:0.1043, training acc: 99.66%, time:33543.62s\n",
            "1265 epoch, D loss:0.0492, G loss:0.1045, training acc: 99.13%, time:33570.29s\n",
            "1266 epoch, D loss:0.0493, G loss:0.1041, training acc: 99.35%, time:33596.33s\n",
            "1267 epoch, D loss:0.0491, G loss:0.1040, training acc: 99.72%, time:33622.28s\n",
            "1268 epoch, D loss:0.0489, G loss:0.1040, training acc: 99.85%, time:33648.35s\n",
            "1269 epoch, D loss:0.0494, G loss:0.1042, training acc: 99.54%, time:33674.92s\n",
            "1270 epoch, D loss:0.0493, G loss:0.1039, training acc: 99.63%, time:33700.95s\n",
            "test acc: 73.19%, time:33703.53s\n",
            "1271 epoch, D loss:0.0491, G loss:0.1037, training acc: 99.50%, time:33729.42s\n",
            "1272 epoch, D loss:0.0490, G loss:0.1043, training acc: 99.94%, time:33755.46s\n",
            "1273 epoch, D loss:0.0484, G loss:0.1050, training acc: 99.81%, time:33782.04s\n",
            "1274 epoch, D loss:0.0485, G loss:0.1053, training acc: 99.47%, time:33808.19s\n",
            "1275 epoch, D loss:0.0482, G loss:0.1051, training acc: 99.66%, time:33834.17s\n",
            "1276 epoch, D loss:0.0487, G loss:0.1042, training acc: 99.54%, time:33860.36s\n",
            "1277 epoch, D loss:0.0489, G loss:0.1041, training acc: 99.41%, time:33887.07s\n",
            "1278 epoch, D loss:0.0492, G loss:0.1041, training acc: 99.75%, time:33913.06s\n",
            "1279 epoch, D loss:0.0491, G loss:0.1039, training acc: 99.57%, time:33939.01s\n",
            "1280 epoch, D loss:0.0492, G loss:0.1043, training acc: 99.54%, time:33965.11s\n",
            "test acc: 74.72%, time:33967.69s\n",
            "1281 epoch, D loss:0.0490, G loss:0.1050, training acc: 99.88%, time:33994.41s\n",
            "1282 epoch, D loss:0.0487, G loss:0.1057, training acc: 99.75%, time:34020.37s\n",
            "1283 epoch, D loss:0.0482, G loss:0.1060, training acc: 99.78%, time:34046.45s\n",
            "1284 epoch, D loss:0.0481, G loss:0.1059, training acc: 99.63%, time:34072.52s\n",
            "1285 epoch, D loss:0.0477, G loss:0.1060, training acc: 99.54%, time:34099.07s\n",
            "1286 epoch, D loss:0.0477, G loss:0.1059, training acc: 99.63%, time:34125.29s\n",
            "1287 epoch, D loss:0.0479, G loss:0.1054, training acc: 99.13%, time:34151.34s\n",
            "1288 epoch, D loss:0.0484, G loss:0.1053, training acc: 99.32%, time:34177.30s\n",
            "1289 epoch, D loss:0.0483, G loss:0.1049, training acc: 99.57%, time:34203.86s\n",
            "1290 epoch, D loss:0.0484, G loss:0.1048, training acc: 99.04%, time:34229.94s\n",
            "test acc: 74.16%, time:34232.52s\n",
            "1291 epoch, D loss:0.0489, G loss:0.1043, training acc: 99.35%, time:34258.56s\n",
            "1292 epoch, D loss:0.0488, G loss:0.1045, training acc: 99.10%, time:34284.62s\n",
            "1293 epoch, D loss:0.0491, G loss:0.1047, training acc: 99.50%, time:34311.28s\n",
            "1294 epoch, D loss:0.0489, G loss:0.1050, training acc: 99.54%, time:34337.20s\n",
            "1295 epoch, D loss:0.0492, G loss:0.1055, training acc: 99.50%, time:34363.32s\n",
            "1296 epoch, D loss:0.0490, G loss:0.1056, training acc: 99.72%, time:34389.33s\n",
            "1297 epoch, D loss:0.0491, G loss:0.1059, training acc: 99.54%, time:34415.78s\n",
            "1298 epoch, D loss:0.0489, G loss:0.1065, training acc: 99.54%, time:34441.68s\n",
            "1299 epoch, D loss:0.0489, G loss:0.1063, training acc: 99.78%, time:34467.67s\n",
            "1300 epoch, D loss:0.0488, G loss:0.1061, training acc: 99.69%, time:34493.56s\n",
            "test acc: 74.44%, time:34496.15s\n",
            "1301 epoch, D loss:0.0484, G loss:0.1063, training acc: 99.85%, time:34522.79s\n",
            "1302 epoch, D loss:0.0482, G loss:0.1061, training acc: 99.69%, time:34548.93s\n",
            "1303 epoch, D loss:0.0483, G loss:0.1058, training acc: 99.35%, time:34575.05s\n",
            "1304 epoch, D loss:0.0485, G loss:0.1062, training acc: 99.60%, time:34601.07s\n",
            "1305 epoch, D loss:0.0485, G loss:0.1060, training acc: 99.88%, time:34627.73s\n",
            "1306 epoch, D loss:0.0484, G loss:0.1058, training acc: 99.72%, time:34653.83s\n",
            "1307 epoch, D loss:0.0487, G loss:0.1065, training acc: 99.47%, time:34679.90s\n",
            "1308 epoch, D loss:0.0487, G loss:0.1062, training acc: 99.63%, time:34705.93s\n",
            "1309 epoch, D loss:0.0483, G loss:0.1061, training acc: 99.72%, time:34732.55s\n",
            "1310 epoch, D loss:0.0482, G loss:0.1058, training acc: 99.72%, time:34758.61s\n",
            "test acc: 76.47%, time:34761.21s\n",
            "1311 epoch, D loss:0.0492, G loss:0.1057, training acc: 99.54%, time:34787.20s\n",
            "1312 epoch, D loss:0.0491, G loss:0.1053, training acc: 99.72%, time:34813.30s\n",
            "1313 epoch, D loss:0.0492, G loss:0.1054, training acc: 99.63%, time:34839.98s\n",
            "1314 epoch, D loss:0.0488, G loss:0.1057, training acc: 99.78%, time:34866.09s\n",
            "1315 epoch, D loss:0.0486, G loss:0.1063, training acc:100.00%, time:34892.18s\n",
            "1316 epoch, D loss:0.0483, G loss:0.1068, training acc: 99.81%, time:34918.24s\n",
            "1317 epoch, D loss:0.0482, G loss:0.1062, training acc: 99.88%, time:34944.89s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1318 epoch, D loss:0.0485, G loss:0.1060, training acc: 99.54%, time:34970.98s\n",
            "1319 epoch, D loss:0.0484, G loss:0.1063, training acc: 99.35%, time:34997.10s\n",
            "1320 epoch, D loss:0.0481, G loss:0.1062, training acc: 98.95%, time:35023.17s\n",
            "test acc: 75.23%, time:35025.77s\n",
            "1321 epoch, D loss:0.0475, G loss:0.1059, training acc: 99.54%, time:35052.33s\n",
            "1322 epoch, D loss:0.0478, G loss:0.1062, training acc: 99.60%, time:35078.71s\n",
            "1323 epoch, D loss:0.0474, G loss:0.1057, training acc: 99.78%, time:35104.93s\n",
            "1324 epoch, D loss:0.0479, G loss:0.1058, training acc: 99.72%, time:35131.08s\n",
            "1325 epoch, D loss:0.0476, G loss:0.1057, training acc: 99.66%, time:35157.65s\n",
            "1326 epoch, D loss:0.0481, G loss:0.1053, training acc: 99.66%, time:35183.54s\n",
            "1327 epoch, D loss:0.0476, G loss:0.1053, training acc: 99.88%, time:35209.50s\n",
            "1328 epoch, D loss:0.0476, G loss:0.1055, training acc: 99.47%, time:35235.54s\n",
            "1329 epoch, D loss:0.0479, G loss:0.1058, training acc: 99.94%, time:35262.08s\n",
            "1330 epoch, D loss:0.0477, G loss:0.1058, training acc: 99.78%, time:35288.18s\n",
            "test acc: 75.56%, time:35290.76s\n",
            "1331 epoch, D loss:0.0477, G loss:0.1053, training acc: 99.75%, time:35316.78s\n",
            "1332 epoch, D loss:0.0478, G loss:0.1052, training acc: 99.66%, time:35342.71s\n",
            "1333 epoch, D loss:0.0481, G loss:0.1053, training acc: 99.54%, time:35369.21s\n",
            "1334 epoch, D loss:0.0481, G loss:0.1059, training acc: 99.69%, time:35395.12s\n",
            "1335 epoch, D loss:0.0480, G loss:0.1058, training acc: 99.69%, time:35421.16s\n",
            "1336 epoch, D loss:0.0482, G loss:0.1052, training acc: 99.63%, time:35447.16s\n",
            "1337 epoch, D loss:0.0489, G loss:0.1048, training acc: 99.29%, time:35473.67s\n",
            "1338 epoch, D loss:0.0491, G loss:0.1049, training acc: 99.50%, time:35499.64s\n",
            "1339 epoch, D loss:0.0494, G loss:0.1042, training acc: 99.66%, time:35525.66s\n",
            "1340 epoch, D loss:0.0494, G loss:0.1043, training acc: 99.57%, time:35551.66s\n",
            "test acc: 75.84%, time:35554.24s\n",
            "1341 epoch, D loss:0.0489, G loss:0.1051, training acc: 99.85%, time:35580.73s\n",
            "1342 epoch, D loss:0.0477, G loss:0.1066, training acc: 99.85%, time:35606.72s\n",
            "1343 epoch, D loss:0.0478, G loss:0.1067, training acc: 99.97%, time:35632.53s\n",
            "1344 epoch, D loss:0.0478, G loss:0.1065, training acc: 99.91%, time:35658.45s\n",
            "1345 epoch, D loss:0.0480, G loss:0.1062, training acc: 99.47%, time:35684.93s\n",
            "1346 epoch, D loss:0.0480, G loss:0.1060, training acc: 99.69%, time:35710.79s\n",
            "1347 epoch, D loss:0.0485, G loss:0.1056, training acc: 99.85%, time:35736.79s\n",
            "1348 epoch, D loss:0.0485, G loss:0.1054, training acc: 99.85%, time:35762.76s\n",
            "1349 epoch, D loss:0.0484, G loss:0.1055, training acc: 99.88%, time:35789.33s\n",
            "1350 epoch, D loss:0.0484, G loss:0.1061, training acc: 99.60%, time:35815.40s\n",
            "test acc: 74.26%, time:35817.99s\n",
            "1351 epoch, D loss:0.0480, G loss:0.1066, training acc: 99.63%, time:35843.89s\n",
            "1352 epoch, D loss:0.0476, G loss:0.1067, training acc: 99.69%, time:35869.80s\n",
            "1353 epoch, D loss:0.0475, G loss:0.1066, training acc: 99.78%, time:35896.20s\n",
            "1354 epoch, D loss:0.0475, G loss:0.1066, training acc: 99.60%, time:35922.07s\n",
            "1355 epoch, D loss:0.0477, G loss:0.1061, training acc: 99.72%, time:35948.09s\n",
            "1356 epoch, D loss:0.0480, G loss:0.1059, training acc: 99.75%, time:35974.08s\n",
            "1357 epoch, D loss:0.0481, G loss:0.1056, training acc: 99.75%, time:36000.56s\n",
            "1358 epoch, D loss:0.0483, G loss:0.1054, training acc: 99.63%, time:36026.41s\n",
            "1359 epoch, D loss:0.0482, G loss:0.1054, training acc: 99.60%, time:36052.41s\n",
            "1360 epoch, D loss:0.0483, G loss:0.1055, training acc: 99.54%, time:36078.29s\n",
            "test acc: 75.72%, time:36080.87s\n",
            "1361 epoch, D loss:0.0479, G loss:0.1056, training acc: 99.81%, time:36107.35s\n",
            "1362 epoch, D loss:0.0477, G loss:0.1062, training acc: 99.72%, time:36133.34s\n",
            "1363 epoch, D loss:0.0481, G loss:0.1061, training acc: 99.91%, time:36159.37s\n",
            "1364 epoch, D loss:0.0482, G loss:0.1062, training acc: 99.97%, time:36185.35s\n",
            "1365 epoch, D loss:0.0484, G loss:0.1057, training acc: 99.88%, time:36211.92s\n",
            "1366 epoch, D loss:0.0485, G loss:0.1051, training acc: 99.91%, time:36237.85s\n",
            "1367 epoch, D loss:0.0480, G loss:0.1054, training acc: 99.85%, time:36263.72s\n",
            "1368 epoch, D loss:0.0481, G loss:0.1056, training acc: 99.97%, time:36289.62s\n",
            "1369 epoch, D loss:0.0482, G loss:0.1057, training acc: 99.78%, time:36316.00s\n",
            "1370 epoch, D loss:0.0479, G loss:0.1058, training acc: 99.94%, time:36341.98s\n",
            "test acc: 76.42%, time:36344.56s\n",
            "1371 epoch, D loss:0.0478, G loss:0.1059, training acc: 99.78%, time:36370.50s\n",
            "1372 epoch, D loss:0.0482, G loss:0.1058, training acc: 99.75%, time:36396.40s\n",
            "1373 epoch, D loss:0.0480, G loss:0.1056, training acc: 99.85%, time:36422.84s\n",
            "1374 epoch, D loss:0.0483, G loss:0.1051, training acc: 99.85%, time:36448.75s\n",
            "1375 epoch, D loss:0.0483, G loss:0.1051, training acc: 99.44%, time:36474.63s\n",
            "1376 epoch, D loss:0.0487, G loss:0.1047, training acc: 99.75%, time:36500.59s\n",
            "1377 epoch, D loss:0.0485, G loss:0.1054, training acc: 99.88%, time:36527.02s\n",
            "1378 epoch, D loss:0.0482, G loss:0.1053, training acc: 99.81%, time:36552.89s\n",
            "1379 epoch, D loss:0.0476, G loss:0.1053, training acc: 99.85%, time:36578.89s\n",
            "1380 epoch, D loss:0.0478, G loss:0.1055, training acc: 99.94%, time:36604.81s\n",
            "test acc: 75.65%, time:36607.43s\n",
            "1381 epoch, D loss:0.0477, G loss:0.1054, training acc: 99.88%, time:36633.88s\n",
            "1382 epoch, D loss:0.0478, G loss:0.1058, training acc: 99.94%, time:36659.85s\n",
            "1383 epoch, D loss:0.0481, G loss:0.1053, training acc: 99.78%, time:36685.94s\n",
            "1384 epoch, D loss:0.0483, G loss:0.1055, training acc: 99.88%, time:36711.96s\n",
            "1385 epoch, D loss:0.0482, G loss:0.1053, training acc: 99.94%, time:36738.43s\n",
            "1386 epoch, D loss:0.0483, G loss:0.1062, training acc: 99.75%, time:36764.27s\n",
            "1387 epoch, D loss:0.0478, G loss:0.1060, training acc: 99.75%, time:36790.14s\n",
            "1388 epoch, D loss:0.0482, G loss:0.1061, training acc: 99.91%, time:36816.02s\n",
            "1389 epoch, D loss:0.0480, G loss:0.1064, training acc: 99.85%, time:36842.37s\n",
            "1390 epoch, D loss:0.0482, G loss:0.1064, training acc: 99.63%, time:36868.16s\n",
            "test acc: 74.67%, time:36870.71s\n",
            "1391 epoch, D loss:0.0481, G loss:0.1057, training acc: 99.78%, time:36896.54s\n",
            "1392 epoch, D loss:0.0483, G loss:0.1058, training acc: 99.85%, time:36922.44s\n",
            "1393 epoch, D loss:0.0481, G loss:0.1062, training acc: 99.85%, time:36948.93s\n",
            "1394 epoch, D loss:0.0478, G loss:0.1060, training acc:100.00%, time:36974.88s\n",
            "1395 epoch, D loss:0.0476, G loss:0.1061, training acc: 99.91%, time:37000.65s\n",
            "1396 epoch, D loss:0.0480, G loss:0.1059, training acc: 99.81%, time:37026.54s\n",
            "1397 epoch, D loss:0.0478, G loss:0.1056, training acc: 99.66%, time:37053.16s\n",
            "1398 epoch, D loss:0.0481, G loss:0.1053, training acc: 99.94%, time:37079.19s\n",
            "1399 epoch, D loss:0.0480, G loss:0.1062, training acc: 99.75%, time:37105.33s\n",
            "1400 epoch, D loss:0.0485, G loss:0.1060, training acc: 99.85%, time:37131.56s\n",
            "test acc: 77.05%, time:37134.14s\n",
            "1401 epoch, D loss:0.0479, G loss:0.1059, training acc: 99.91%, time:37160.72s\n",
            "1402 epoch, D loss:0.0482, G loss:0.1057, training acc: 99.85%, time:37186.91s\n",
            "1403 epoch, D loss:0.0479, G loss:0.1053, training acc: 99.81%, time:37212.99s\n",
            "1404 epoch, D loss:0.0486, G loss:0.1052, training acc: 99.81%, time:37238.95s\n",
            "1405 epoch, D loss:0.0489, G loss:0.1052, training acc: 99.69%, time:37265.47s\n",
            "1406 epoch, D loss:0.0488, G loss:0.1055, training acc: 99.97%, time:37291.70s\n",
            "1407 epoch, D loss:0.0486, G loss:0.1057, training acc: 99.88%, time:37317.66s\n",
            "1408 epoch, D loss:0.0488, G loss:0.1066, training acc: 99.97%, time:37343.73s\n",
            "1409 epoch, D loss:0.0480, G loss:0.1069, training acc: 99.91%, time:37370.47s\n",
            "1410 epoch, D loss:0.0483, G loss:0.1064, training acc: 99.66%, time:37396.81s\n",
            "test acc: 71.09%, time:37399.39s\n",
            "1411 epoch, D loss:0.0489, G loss:0.1052, training acc: 99.57%, time:37425.77s\n",
            "1412 epoch, D loss:0.0488, G loss:0.1051, training acc: 99.63%, time:37452.07s\n",
            "1413 epoch, D loss:0.0488, G loss:0.1050, training acc: 99.97%, time:37479.00s\n",
            "1414 epoch, D loss:0.0483, G loss:0.1048, training acc: 99.91%, time:37505.31s\n",
            "1415 epoch, D loss:0.0483, G loss:0.1052, training acc: 99.91%, time:37531.69s\n",
            "1416 epoch, D loss:0.0478, G loss:0.1049, training acc: 99.94%, time:37557.84s\n",
            "1417 epoch, D loss:0.0484, G loss:0.1048, training acc: 99.75%, time:37584.65s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1418 epoch, D loss:0.0480, G loss:0.1044, training acc: 99.75%, time:37610.96s\n",
            "1419 epoch, D loss:0.0489, G loss:0.1046, training acc: 99.60%, time:37637.34s\n",
            "1420 epoch, D loss:0.0491, G loss:0.1050, training acc: 99.47%, time:37663.53s\n",
            "test acc: 75.30%, time:37666.12s\n",
            "1421 epoch, D loss:0.0487, G loss:0.1055, training acc: 99.10%, time:37693.05s\n",
            "1422 epoch, D loss:0.0486, G loss:0.1060, training acc: 99.69%, time:37719.27s\n",
            "1423 epoch, D loss:0.0490, G loss:0.1074, training acc: 99.32%, time:37745.53s\n",
            "1424 epoch, D loss:0.0487, G loss:0.1062, training acc: 99.85%, time:37771.73s\n",
            "1425 epoch, D loss:0.0484, G loss:0.1063, training acc: 99.81%, time:37798.61s\n",
            "1426 epoch, D loss:0.0483, G loss:0.1067, training acc: 99.81%, time:37824.66s\n",
            "1427 epoch, D loss:0.0478, G loss:0.1071, training acc: 99.75%, time:37850.98s\n",
            "1428 epoch, D loss:0.0473, G loss:0.1073, training acc: 99.75%, time:37877.32s\n",
            "1429 epoch, D loss:0.0481, G loss:0.1061, training acc: 99.78%, time:37904.21s\n",
            "1430 epoch, D loss:0.0484, G loss:0.1056, training acc: 99.54%, time:37930.60s\n",
            "test acc: 75.51%, time:37933.19s\n",
            "1431 epoch, D loss:0.0486, G loss:0.1056, training acc: 99.81%, time:37959.53s\n",
            "1432 epoch, D loss:0.0482, G loss:0.1060, training acc:100.00%, time:37985.78s\n",
            "1433 epoch, D loss:0.0478, G loss:0.1066, training acc: 99.91%, time:38012.66s\n",
            "1434 epoch, D loss:0.0477, G loss:0.1068, training acc: 99.94%, time:38038.93s\n",
            "1435 epoch, D loss:0.0481, G loss:0.1068, training acc: 99.69%, time:38065.29s\n",
            "1436 epoch, D loss:0.0479, G loss:0.1066, training acc: 99.78%, time:38091.66s\n",
            "1437 epoch, D loss:0.0477, G loss:0.1062, training acc: 99.57%, time:38118.51s\n",
            "1438 epoch, D loss:0.0480, G loss:0.1060, training acc: 99.60%, time:38144.84s\n",
            "1439 epoch, D loss:0.0485, G loss:0.1055, training acc: 99.69%, time:38171.21s\n",
            "1440 epoch, D loss:0.0478, G loss:0.1054, training acc: 99.81%, time:38197.68s\n",
            "test acc: 76.28%, time:38200.27s\n",
            "1441 epoch, D loss:0.0484, G loss:0.1050, training acc: 99.97%, time:38227.20s\n",
            "1442 epoch, D loss:0.0484, G loss:0.1054, training acc: 99.85%, time:38253.69s\n",
            "1443 epoch, D loss:0.0486, G loss:0.1051, training acc: 99.97%, time:38279.95s\n",
            "1444 epoch, D loss:0.0483, G loss:0.1054, training acc: 99.97%, time:38306.36s\n",
            "1445 epoch, D loss:0.0484, G loss:0.1056, training acc: 99.78%, time:38333.19s\n",
            "1446 epoch, D loss:0.0483, G loss:0.1051, training acc: 99.26%, time:38359.66s\n",
            "1447 epoch, D loss:0.0479, G loss:0.1054, training acc: 99.60%, time:38386.03s\n",
            "1448 epoch, D loss:0.0482, G loss:0.1055, training acc: 99.57%, time:38412.23s\n",
            "1449 epoch, D loss:0.0482, G loss:0.1057, training acc: 99.57%, time:38439.15s\n",
            "1450 epoch, D loss:0.0483, G loss:0.1055, training acc: 99.78%, time:38465.66s\n",
            "test acc: 72.47%, time:38468.25s\n",
            "1451 epoch, D loss:0.0484, G loss:0.1057, training acc: 99.85%, time:38494.59s\n",
            "1452 epoch, D loss:0.0484, G loss:0.1061, training acc: 99.88%, time:38520.83s\n",
            "1453 epoch, D loss:0.0478, G loss:0.1067, training acc: 99.75%, time:38547.63s\n",
            "1454 epoch, D loss:0.0475, G loss:0.1070, training acc: 99.78%, time:38574.19s\n",
            "1455 epoch, D loss:0.0472, G loss:0.1066, training acc: 99.91%, time:38600.35s\n",
            "1456 epoch, D loss:0.0470, G loss:0.1064, training acc: 99.91%, time:38626.66s\n",
            "1457 epoch, D loss:0.0474, G loss:0.1058, training acc: 99.97%, time:38653.56s\n",
            "1458 epoch, D loss:0.0475, G loss:0.1055, training acc: 99.85%, time:38680.05s\n",
            "1459 epoch, D loss:0.0481, G loss:0.1050, training acc: 99.44%, time:38706.43s\n",
            "1460 epoch, D loss:0.0484, G loss:0.1045, training acc: 99.63%, time:38732.67s\n",
            "test acc: 76.12%, time:38735.26s\n",
            "1461 epoch, D loss:0.0485, G loss:0.1048, training acc: 99.88%, time:38762.21s\n",
            "1462 epoch, D loss:0.0486, G loss:0.1054, training acc: 99.94%, time:38788.41s\n",
            "1463 epoch, D loss:0.0483, G loss:0.1053, training acc: 99.94%, time:38814.87s\n",
            "1464 epoch, D loss:0.0483, G loss:0.1054, training acc: 99.91%, time:38841.22s\n",
            "1465 epoch, D loss:0.0484, G loss:0.1054, training acc: 99.91%, time:38868.14s\n",
            "1466 epoch, D loss:0.0484, G loss:0.1053, training acc: 99.94%, time:38894.57s\n",
            "1467 epoch, D loss:0.0483, G loss:0.1049, training acc: 99.94%, time:38920.79s\n",
            "1468 epoch, D loss:0.0486, G loss:0.1047, training acc: 99.85%, time:38947.25s\n",
            "1469 epoch, D loss:0.0487, G loss:0.1047, training acc: 99.91%, time:38974.34s\n",
            "1470 epoch, D loss:0.0493, G loss:0.1042, training acc: 99.75%, time:39000.73s\n",
            "test acc: 74.09%, time:39003.32s\n",
            "1471 epoch, D loss:0.0489, G loss:0.1042, training acc: 99.69%, time:39029.52s\n",
            "1472 epoch, D loss:0.0492, G loss:0.1046, training acc: 99.91%, time:39055.85s\n",
            "1473 epoch, D loss:0.0489, G loss:0.1052, training acc: 99.75%, time:39082.87s\n",
            "1474 epoch, D loss:0.0483, G loss:0.1056, training acc: 99.88%, time:39109.29s\n",
            "1475 epoch, D loss:0.0474, G loss:0.1057, training acc: 99.85%, time:39135.53s\n",
            "1476 epoch, D loss:0.0480, G loss:0.1053, training acc: 99.72%, time:39162.55s\n",
            "1477 epoch, D loss:0.0476, G loss:0.1052, training acc: 99.69%, time:39189.87s\n",
            "1478 epoch, D loss:0.0477, G loss:0.1046, training acc: 99.63%, time:39216.31s\n",
            "1479 epoch, D loss:0.0480, G loss:0.1043, training acc: 99.69%, time:39242.61s\n",
            "1480 epoch, D loss:0.0483, G loss:0.1050, training acc: 99.69%, time:39269.33s\n",
            "test acc: 74.28%, time:39271.91s\n",
            "1481 epoch, D loss:0.0485, G loss:0.1056, training acc: 99.60%, time:39298.73s\n",
            "1482 epoch, D loss:0.0481, G loss:0.1059, training acc: 99.75%, time:39325.06s\n",
            "1483 epoch, D loss:0.0478, G loss:0.1064, training acc: 99.78%, time:39351.39s\n",
            "1484 epoch, D loss:0.0479, G loss:0.1065, training acc: 99.63%, time:39377.68s\n",
            "1485 epoch, D loss:0.0479, G loss:0.1063, training acc: 99.63%, time:39404.58s\n",
            "1486 epoch, D loss:0.0481, G loss:0.1063, training acc: 99.81%, time:39430.89s\n",
            "1487 epoch, D loss:0.0482, G loss:0.1057, training acc: 99.69%, time:39457.18s\n",
            "1488 epoch, D loss:0.0482, G loss:0.1058, training acc: 99.66%, time:39483.44s\n",
            "1489 epoch, D loss:0.0483, G loss:0.1062, training acc: 99.41%, time:39510.25s\n",
            "1490 epoch, D loss:0.0482, G loss:0.1066, training acc: 99.66%, time:39536.54s\n",
            "test acc: 77.51%, time:39539.13s\n",
            "1491 epoch, D loss:0.0477, G loss:0.1064, training acc: 98.58%, time:39565.36s\n",
            "1492 epoch, D loss:0.0481, G loss:0.1068, training acc: 97.99%, time:39591.79s\n",
            "1493 epoch, D loss:0.0479, G loss:0.1059, training acc: 99.57%, time:39618.74s\n",
            "1494 epoch, D loss:0.0476, G loss:0.1057, training acc: 99.85%, time:39645.12s\n",
            "1495 epoch, D loss:0.0479, G loss:0.1058, training acc: 99.97%, time:39671.61s\n",
            "1496 epoch, D loss:0.0478, G loss:0.1059, training acc: 99.88%, time:39697.91s\n",
            "1497 epoch, D loss:0.0479, G loss:0.1056, training acc: 99.88%, time:39724.75s\n",
            "1498 epoch, D loss:0.0478, G loss:0.1058, training acc: 99.81%, time:39751.12s\n",
            "1499 epoch, D loss:0.0482, G loss:0.1058, training acc: 99.88%, time:39777.54s\n",
            "1500 epoch, D loss:0.0488, G loss:0.1058, training acc: 99.66%, time:39803.88s\n",
            "test acc: 74.35%, time:39806.47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P4xMgyiWxXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.eval()\n",
        "M = 10\n",
        "net = torch.load(\"Bayesian_GC_LSTM_86.pkl\").to(device)\n",
        "correct = 0\n",
        "for (data, label, num_frame) in test_loader:\n",
        "    data, label, num_frame = data.to(device), label.to(device), num_frame.to(device)\n",
        "    for _ in range(M):\n",
        "        output = net(data,num_frame)\n",
        "        _, pred = output.max(1)\n",
        "        correct += pred.eq(label).sum().item()\n",
        "print(\"test acc: {:5.2f}%\".format((correct/M)/len(test_dataset)*100.))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}